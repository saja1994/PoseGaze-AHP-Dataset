{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic\n",
        "!pip install PyMuPDF\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poErdfssI32z",
        "outputId": "18176bf1-62b6-42f8-8dc8-9baa7324e29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.10/dist-packages (0.39.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.23.4)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.13)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import re\n",
        "from pathlib import Path\n",
        "from anthropic import Anthropic\n",
        "from pypdf import PdfReader\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "RoQunyvhjJcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCRY0cLlwCrm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import anthropic\n",
        "\n",
        "# Set the API key directly in the environment (optional, if not set externally)\n",
        "os.environ['ANTHROPIC_API_KEY'] ='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'\n",
        "#os.environ['ANTHROPIC_API_KEY'] = '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' ## Dr.Sherzod Key\n",
        "\n",
        "client = anthropic.Anthropic(\n",
        "    api_key=os.environ['ANTHROPIC_API_KEY'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#new\n",
        "\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler('extraction.log')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Initialize the Anthropic client\n",
        "client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
        "MODEL_NAME = \"claude-3-sonnet-20240229\"\n",
        "\n",
        "\n",
        "def format_number(value: float) -> str:\n",
        "    \"\"\"Format number consistently: whole numbers as integers, decimals without trailing zeros\"\"\"\n",
        "    if pd.isna(value) or value is None:\n",
        "        return \"0\"\n",
        "    try:\n",
        "        if float(value).is_integer():\n",
        "            return str(int(value))\n",
        "        return str(float(value)).rstrip('0').rstrip('.')\n",
        "    except ValueError:\n",
        "        return str(value)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str):\n",
        "    \"\"\"Extract text content from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        text = ' '.join(page.extract_text() for page in reader.pages)\n",
        "        if not text.strip():\n",
        "            logging.warning(f\"No text extracted from {pdf_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "def read_prompt(prompt_path: str):\n",
        "    \"\"\"Read the prompt from file.\"\"\"\n",
        "    try:\n",
        "        with open(prompt_path, \"r\", encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reading prompt file: {e}\")\n",
        "        raise\n",
        "\n",
        "def ask_question(context, prompt):\n",
        "    \"\"\"Send request to Claude and get response.\"\"\"\n",
        "    try:\n",
        "        complete_prompt = f\"{prompt}\\n\\nContext: {context}\\n\\nPlease extract all relevant metadata from the research paper.\"\n",
        "        messages = [{\"role\": \"user\", \"content\": complete_prompt}]\n",
        "\n",
        "        response = client.messages.create(\n",
        "            model=MODEL_NAME,\n",
        "            max_tokens=1000,\n",
        "            messages=messages,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        raw_content = response.content[0].text\n",
        "        print(f\"Raw response from Claude: {raw_content}\")\n",
        "        return raw_content\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error querying Claude: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def clean_json_output(response_text):\n",
        "    \"\"\"Clean and structure the complete JSON output from Claude's response.\"\"\"\n",
        "    try:\n",
        "        # Find JSON content\n",
        "        json_start = response_text.find('{')\n",
        "        json_end = response_text.rfind('}')\n",
        "\n",
        "        if json_start == -1 or json_end == -1:\n",
        "            logging.error(\"No JSON structure found in response\")\n",
        "            return {}\n",
        "\n",
        "        json_content = response_text[json_start:json_end + 1]\n",
        "        return json.loads(json_content)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in clean_json_output: {e}\")\n",
        "        logging.debug(f\"Raw response text: {response_text}\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "bDYC2LZqIwwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import re\n",
        "from pathlib import Path\n",
        "from anthropic import Anthropic\n",
        "from pypdf import PdfReader\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler('extraction.log')\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def standardize_disease_names(diagnose):\n",
        "    \"\"\"Enhanced standardization for disease names\"\"\"\n",
        "    if pd.isna(diagnose):\n",
        "        return ''\n",
        "\n",
        "    # Convert to lowercase and clean up\n",
        "    diagnose = str(diagnose).lower().strip()\n",
        "\n",
        "    diagnose = str(diagnose).lower().strip()\n",
        "    diagnose = re.sub(r'\\s*\\([^)]*\\)', '', diagnose)  # Remove parentheses and contents\n",
        "    diagnose = re.sub(r',?\\s*type\\s+[IiVv1234]+', '', diagnose)  # Remove type specifications\n",
        "    diagnose = diagnose.replace(\"'s\", \"\")  # Remove possessives\n",
        "    diagnose = re.sub(r'\\s+left eye', '', diagnose)  # Remove eye specifications\n",
        "    diagnose = re.sub(r'\\s+right eye', '', diagnose)\n",
        "\n",
        "\n",
        "    # Updated comprehensive mapping dictionary - REVERSED to match ground truth\n",
        "    standardization_dict = {\n",
        "        \"duane syndrome\": [  # Changed from \"duane retraction syndrome\"\n",
        "            \"drs\", \"duane retraction syndrome (drs)\", \"duane retraction syndrome\",\n",
        "            \"duane's syndrome\", \"duane retraction syndrome type 1\", \"duane retraction syndrome type 2\",\n",
        "            \"esotropic duane syndrome\", \"exotropic duane syndrome\",\n",
        "            \"duane syndrome, type 4\", \"duane's syndrome, type i\",\n",
        "            \"lateral rectus muscle palsy and type i duane syndrome\", \"duane retraction syndrome (drs) type ii\",\n",
        "            \"duane retraction syndrome (drs) type 2\", \"duane retraction syndrome (drs) type 1\",\n",
        "            \"duane syndrome type i\", \"esotropic duane retraction syndrome\", \"type 4, duane retraction syndrome\",\n",
        "            \"type i duane syndrome\", \"duane syndrome type 1\", \"unilateral duane's syndrome, type i\",\n",
        "            \"esotropic duane syndrome and vi nerve palsy\", \"type i, unilateral duane syndrome\",\n",
        "            \"unilateral duane syndrome, type i\", \"duane's syndrome, type i\"\n",
        "        ],\n",
        "        \"superior oblique palsy\": [\n",
        "            \"superior oblique palsy (sop)\", \"congenital superior oblique palsy\",\n",
        "            \"unilateral congenital superior oblique palsy\", \"unilateral superior oblique palsy\",\n",
        "            \"unilateral superior oblique (so) palsy\", \"traumatic superior oblique palsy\",\n",
        "            \"congenital superior oblique palsy (csop)\", \"superior oblique palsy (so) hypofunction\",\n",
        "            \"unilateral congenital superior oblique palsy (sop)\", \"congenital superior oblique palsy (sop)\",\n",
        "            \"unilateral superior oblique palsy (sop)\"\n",
        "        ],\n",
        "        \"monocular elevation deficiency\": [\n",
        "            \"med\", \"monocular elevation deficiency (med)\", \"monocular elevation deﬁciency\",\n",
        "            \"monocular elevation deﬁciency (med)\"\n",
        "        ],\n",
        "        \"brown syndrome\": [\n",
        "            \"acquired brown's syndrome\", \"congenital brown syndrome\",\n",
        "            \"brown's syndrome\", \"brown syndrome\", \"acquired brown syndrome\"\n",
        "        ],\n",
        "        \"nystagmus\": [\n",
        "            \"congenital motor nystagmus (cmn)\", \"infantile nystagmus syndrome (ins)\",\n",
        "            \"idiopathic infantile nystagmus (iin)\", \"spasmus nutans\",\n",
        "            \"infantile nystagmus syndrome\", \"congenital nystagmus\",\n",
        "            \"sensory defect nystagmus (sdn)\", \"infantile idiopathic nystagmus (iin)\",\n",
        "            \"nystagmus blockage syndrome\", \"congenital nystagmus\",\n",
        "            \"nystagmus with anomalous head position (ahp)\",\n",
        "            \"anomalous head position (ahp), nystagmus\"\n",
        "        ],\n",
        "        \"strabismus\": [\n",
        "            \"exotropic duane syndrome\", \"incomitant strabismus\",\n",
        "            \"esotropic duane syndrome\", \"horizontal strabismus\", \"vertical strabismus\",\n",
        "            \"sensory exotropia\", \"various congenital strabismus conditions\"\n",
        "        ],\n",
        "        \"inferior oblique palsy\": [\n",
        "            \"inferior oblique palsy\", \"skew deviation mimicking inferior oblique palsy\",\n",
        "            \"inferior oblique (io) paresis\"\n",
        "        ],\n",
        "        \"ptosis\": [\n",
        "            \"isolated congenital ptosis\", \"ptosis\"\n",
        "        ],\n",
        "        \"superior rectus hypoplasia\": [\n",
        "            \"combined superior rectus hypoplasia and superior oblique palsy\"\n",
        "        ],\n",
        "        \"cranial nerve palsy\": [\n",
        "            \"fourth cranial nerve (cn4) palsy\", \"vi nerve palsy\"\n",
        "        ],\n",
        "        \"double elevator palsy\": [\n",
        "            \"double elevator palsy\", \"double elevator palsy (dep)\"\n",
        "        ],\n",
        "        \"pseudotumor cerebri\": [\n",
        "            \"pseudotumor cerebri\"\n",
        "        ],\n",
        "        \"moebius syndrome\": [\n",
        "            \"moebius syndrome\"\n",
        "        ],\n",
        "        \"dissociated vertical deviation\": [\n",
        "            \"dvd\", \"dissociated vertical deviation (dvd)\"\n",
        "        ],\n",
        "        \"sixth nerve palsy\": [\n",
        "            \"complete sixth nerve palsy\", \"cnsp\", \"complete sixth nerve palsy (cnsp)\"],\n",
        "    }\n",
        "\n",
        "    # Clean up the input diagnose\n",
        "    diagnose = diagnose.strip()\n",
        "\n",
        "    # Try to match with standardized names\n",
        "    for standard_name, variations in standardization_dict.items():\n",
        "        if diagnose in variations:\n",
        "            return standard_name\n",
        "\n",
        "        clean_diagnose = re.sub(r'\\s*\\([^)]*\\)', '', diagnose)\n",
        "        clean_diagnose = re.sub(r',.*$', '', clean_diagnose)\n",
        "        clean_diagnose = re.sub(r'\\s+type\\s+[IiVv1234]+', '', clean_diagnose)\n",
        "\n",
        "        for variation in variations:\n",
        "            if clean_diagnose in variation or variation in clean_diagnose:\n",
        "                return standard_name\n",
        "\n",
        "    return diagnose\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def format_number(number):\n",
        "    \"\"\"Format the number to one decimal place.\"\"\"\n",
        "    return f\"{number:.1f}\"\n",
        "\n",
        "def standardize_degree_values(value: str) -> str:\n",
        "    \"\"\"Standardize degree values while preserving uncertainty notation.\"\"\"\n",
        "\n",
        "    # Handle None or NaN values by returning \"0\"\n",
        "    if pd.isna(value) or value is None:\n",
        "        return \"0\"\n",
        "\n",
        "    # Handle direct numeric values\n",
        "    if isinstance(value, (int, float)):\n",
        "        return f\"{format_number(float(value))}°\"\n",
        "\n",
        "    # Convert the value to a stripped, lowercase string for consistency\n",
        "    value = str(value).strip().lower()\n",
        "\n",
        "    # Handle multiple comma-separated values, keeping only the first one\n",
        "    if ',' in value:\n",
        "        value = value.split(',')[0].strip()\n",
        "\n",
        "    # Remove unwanted text (e.g., ' pd')\n",
        "    value = value.replace(' pd', '')\n",
        "\n",
        "    # Handle values with inequality signs \"<\" or \">\"\n",
        "    if '<' in value:\n",
        "        try:\n",
        "            num = value.replace('<', '').replace('°', '').strip()\n",
        "            return f\"<{format_number(float(num))}°\"\n",
        "        except ValueError:\n",
        "            return value\n",
        "    elif '>' in value:\n",
        "        try:\n",
        "            num = value.replace('>', '').replace('°', '').strip()\n",
        "            return f\">{format_number(float(num))}°\"\n",
        "        except ValueError:\n",
        "            return value\n",
        "\n",
        "    # Handle values with uncertainty notation \"±\"\n",
        "    elif '±' in value:\n",
        "        try:\n",
        "            base, uncertainty = value.split('±')\n",
        "            base_num = format_number(float(base.replace('°', '').strip()))\n",
        "            uncertainty_num = format_number(float(uncertainty.replace('°', '').strip()))\n",
        "            return f\"{base_num}° ± {uncertainty_num}°\"\n",
        "        except ValueError:\n",
        "            return value\n",
        "\n",
        "    # Handle ranges like \"10-20\" or \"10 to 20\" by averaging the range\n",
        "    try:\n",
        "        if '-' in value or ' to ' in value:\n",
        "            parts = value.replace(' to ', '-').split('-')\n",
        "            numbers = [float(p.replace('°', '').strip()) for p in parts]\n",
        "            result = sum(numbers) / len(numbers)  # Average of the range\n",
        "            return f\"{format_number(result)}°\"\n",
        "        else:\n",
        "            # Single numeric value case\n",
        "            result = float(value.replace('°', '').strip())\n",
        "            return f\"{format_number(result)}°\"\n",
        "    except ValueError:\n",
        "        # Return the original value if conversion fails\n",
        "        return value\n",
        "\n",
        "\n",
        "def standardize_eye_misalignment(value: str) -> str:\n",
        "    \"\"\"Enhanced standardization for eye misalignment\"\"\"\n",
        "    if pd.isna(value):\n",
        "        return ''\n",
        "\n",
        "    value = str(value).lower().strip()\n",
        "\n",
        "    conditions = re.split(r'[,\\s]+(?:and|with|,|\\+)', value)\n",
        "\n",
        "    misalignment_mapping = {\n",
        "        'Hypertropia': ['hypertropia', 'ht', 'hyper'],\n",
        "        'Hypotropia': ['hypotropia', 'hot', 'hypo'],\n",
        "        'Esotropia': ['esotropia', 'et', 'eso'],\n",
        "        'Exotropia': ['exotropia', 'xt', 'exo']\n",
        "    }\n",
        "\n",
        "    standardized = []\n",
        "    for condition in conditions:\n",
        "        condition = condition.strip()\n",
        "        if condition:  # Only process non-empty conditions\n",
        "            for standard, variants in misalignment_mapping.items():\n",
        "                if any(variant in condition for variant in variants):\n",
        "                    standardized.append(standard)\n",
        "                    break\n",
        "\n",
        "    return ', '.join(sorted(set(standardized))) if standardized else value\n",
        "\n",
        "def standardize_ahp_type(value: str) -> str:\n",
        "    \"\"\"Enhanced standardization for AHP type\"\"\"\n",
        "    if pd.isna(value):\n",
        "        return ''\n",
        "\n",
        "    value = str(value).lower().strip()\n",
        "\n",
        "    # Split multiple types if present\n",
        "    types = [t.strip() for t in value.split(',')]\n",
        "    standardized_types = []\n",
        "\n",
        "    type_mapping = {\n",
        "        'Face turn': ['face turn', 'face/head turn', 'face turning','head turn', 'head turning'],  # Will match to Face turn\n",
        "        'chin-up': ['chin up', 'chin-up', 'chinup', 'chin elevation'],\n",
        "        'chin-down': ['chin down', 'chin-down', 'chindown', 'chin depression'],\n",
        "        'head tilt': ['head tilt', 'head-tilt', 'headtilt', 'tilt']\n",
        "    }\n",
        "\n",
        "\n",
        "    for t in types:\n",
        "        t = t.strip()\n",
        "        matched = False\n",
        "        for standard_type, variations in type_mapping.items():\n",
        "            if any(var in t for var in variations):\n",
        "                standardized_types.append(standard_type)\n",
        "                matched = True\n",
        "                break\n",
        "        if not matched and t:  # If no match found and not empty\n",
        "            standardized_types.append(t)\n",
        "\n",
        "    return ', '.join(sorted(set(standardized_types))) if standardized_types else value\n",
        "\n",
        "def standardize_eye(value: str) -> str:\n",
        "    \"\"\"Standardize eye values\"\"\"\n",
        "    if pd.isna(value):\n",
        "        return ''\n",
        "\n",
        "    value = str(value).lower().strip()\n",
        "\n",
        "    left_variants = {'l', 'left', 'os', 'left eye'}\n",
        "    right_variants = {'r', 'right', 'od', 'right eye'}\n",
        "    both_variants = {'both', 'ou', 'bilateral', 'both eyes'}\n",
        "\n",
        "    if any(variant == value for variant in left_variants):\n",
        "        return 'left'\n",
        "    elif any(variant == value for variant in right_variants):\n",
        "        return 'right'\n",
        "    elif any(variant == value for variant in both_variants):\n",
        "        return 'both'\n",
        "\n",
        "    return value\n",
        "\n",
        "\n",
        "\n",
        "def standardize_patient_count(value: str) -> str:\n",
        "    \"\"\"Standardize patient count values\"\"\"\n",
        "    if pd.isna(value) or value is None:\n",
        "        return \"0\"\n",
        "\n",
        "    if isinstance(value, (int, float)):\n",
        "        if float(value).is_integer():\n",
        "            return str(int(value))\n",
        "        return str(float(value))\n",
        "\n",
        "    value = str(value).lower().strip()\n",
        "    value = re.sub(r'\\s*patients?\\s*', '', value)\n",
        "\n",
        "    try:\n",
        "        result = float(value)\n",
        "        if result.is_integer():\n",
        "            return str(int(result))\n",
        "        return str(result)\n",
        "    except ValueError:\n",
        "        return \"0\"\n",
        "\n",
        "def standardize_ahp_direction(direction: str, eye: str) -> str:\n",
        "    \"\"\"Standardize AHP direction based on eye information.\"\"\"\n",
        "    if pd.isna(direction) or not direction:\n",
        "        return \"\"\n",
        "\n",
        "    direction = str(direction).lower().strip()\n",
        "    eye = str(eye).lower().strip()\n",
        "\n",
        "    # Handle ipsilateral/contralateral cases\n",
        "    if \"ipsilateral\" in direction or \"same side\" in direction:\n",
        "        return eye if eye in [\"left\", \"right\"] else \"\"\n",
        "    elif \"contralateral\" in direction or \"opposite side\" in direction:\n",
        "        if eye == \"left\":\n",
        "            return \"right\"\n",
        "        elif eye == \"right\":\n",
        "            return \"left\"\n",
        "        return \"\"\n",
        "\n",
        "    # Standard directions\n",
        "    if \"left\" in direction:\n",
        "        return \"left\"\n",
        "    elif \"right\" in direction:\n",
        "        return \"right\"\n",
        "    elif \"up\" in direction or \"superior\" in direction:\n",
        "        return \"upward\"\n",
        "    elif \"down\" in direction or \"inferior\" in direction:\n",
        "        return \"downward\"\n",
        "\n",
        "    return direction\n",
        "\n",
        "def apply_standardization(metadata: dict) -> dict:\n",
        "    \"\"\"Apply all standardizations to metadata\"\"\"\n",
        "    standardized = {\n",
        "        \"PaperTitle\": metadata.get(\"PaperTitle\", \"\"),\n",
        "        \"patientsNo\": standardize_patient_count(metadata.get(\"patientsNo\", \"\")),\n",
        "        \"patients\": []\n",
        "    }\n",
        "\n",
        "    # Handle both patientsINFO and patients keys\n",
        "    patient_list = metadata.get(\"patientsINFO\", metadata.get(\"patients\", []))\n",
        "    if not isinstance(patient_list, list):\n",
        "        patient_list = [patient_list]\n",
        "\n",
        "    for patient in patient_list:\n",
        "        standardized_patient = {\n",
        "            \"Diagnose\": standardize_disease_names(patient.get(\"Diagnose\", \"\")),\n",
        "            \"patientsNoAHP\": standardize_patient_count(patient.get(\"patientsNoAHP\", \"\")),\n",
        "            \"AHPType\": standardize_ahp_type(patient.get(\"AHPType\", \"\")),\n",
        "            \"AHPDirection\": patient.get(\"AHPDirection\", \"\").lower().strip(),\n",
        "            \"AHPDegree\": standardize_degree_values(patient.get(\"AHPDegree\", \"\")),\n",
        "            \"Eye\": standardize_eye(patient.get(\"eye\", patient.get(\"Eye\", \"\"))),\n",
        "            \"EyeMisalignment\": standardize_eye_misalignment(patient.get(\"eyeMisalignment\", patient.get(\"EyeMisalignment\", \"\"))),\n",
        "            \"DegreePD\": standardize_degree_values(patient.get(\"degree\", patient.get(\"DegreePD\", \"\")))\n",
        "        }\n",
        "\n",
        "        standardized[\"patients\"].append(standardized_patient)\n",
        "\n",
        "    return standardized"
      ],
      "metadata": {
        "id": "LsXhnBqUqI8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dynamic_prompt(base_prompt: str, previous_results: list = None) -> str:\n",
        "    \"\"\"Creates a dynamic prompt that adapts based on previous extraction attempts\"\"\"\n",
        "    if not previous_results:\n",
        "        return base_prompt\n",
        "\n",
        "    issues = analyze_extraction_issues(previous_results)\n",
        "    feedback_sections = []\n",
        "\n",
        "    # Add field-specific guidance based on issues\n",
        "    if issues.get('Diagnose', {}).get('count', 0) > 0:\n",
        "        feedback_sections.append(\"\"\"\n",
        "CRITICAL: Multiple Disease Guidelines:\n",
        "- When multiple conditions exist, list ALL separated by comma\n",
        "- Keep diseases separate, do not combine\n",
        "- Do not alter original disease names (e.g., keep 'duane syndrome' as is, do not convert to 'duane retraction syndrome')\n",
        "- Remove type specifications and additional descriptors\n",
        "Examples:\n",
        "- Correct: \"duane syndrome, VI nerve palsy\"\n",
        "- Correct: \"brown syndrome, superior oblique palsy\"\n",
        "- Incorrect: \"duane retraction syndrome\"\n",
        "- Incorrect: \"duane syndrome type 1\\\"\"\"\")\n",
        "\n",
        "    if issues.get('AHPDegree', {}).get('count', 0) > 0 or issues.get('DegreePD', {}).get('count', 0) > 0:\n",
        "        feedback_sections.append(\"\"\"\n",
        "CRITICAL: Measurement Guidelines:\n",
        "- For multiple misalignments, provide ALL corresponding measurements\n",
        "- Maintain one-to-one correspondence between conditions and measurements\n",
        "- Express all values with degree symbol (°)\n",
        "Examples:\n",
        "- eyeMisalignment: \"Esotropia, Hypertropia\"\n",
        "  degree: \"25°, 10°\"\n",
        "- Single value: \"25°\\\"\"\"\")\n",
        "\n",
        "    if issues.get('AHPType', {}).get('count', 0) > 0:\n",
        "        feedback_sections.append(\"\"\"\n",
        "CRITICAL: AHP Type and Direction Guidelines:\n",
        "- Record \"face turn\" instead of \"head turn\"\n",
        "- Maintain exact correspondence between types and directions\n",
        "- List ALL types when multiple exist\n",
        "Examples:\n",
        "- AHPType: \"face turn, chin-up\"\n",
        "  AHPDirection: \"left, upward\"\n",
        "- Single: \"face turn\" with direction \"left\\\"\"\"\")\n",
        "\n",
        "    if issues.get('EyeMisalignment', {}).get('count', 0) > 0:\n",
        "        feedback_sections.append(\"\"\"\n",
        "CRITICAL: Eye Misalignment Guidelines:\n",
        "- List ALL misalignments when multiple exist\n",
        "- Must correspond with degree measurements\n",
        "- Use ONLY: \"Hypertropia\", \"Hypotropia\", \"Esotropia\", \"Exotropia\"\n",
        "Examples:\n",
        "- Multiple: \"Esotropia, Hypertropia\" with degrees \"25°, 10°\"\n",
        "- Single: \"Esotropia\" with degree \"25°\\\"\"\"\")\n",
        "\n",
        "    if issues.get('Eye', {}).get('count', 0) > 0:\n",
        "        feedback_sections.append(\"\"\"\n",
        "CRITICAL: Eye Specification Guidelines:\n",
        "- Use ONLY: \"left\", \"right\", or \"both\"\n",
        "- For bilateral conditions, use \"both\"\n",
        "- Convert all variations (OS -> \"left\", OD -> \"right\", OU -> \"both\")\"\"\")\n",
        "\n",
        "    if issues.get('patientsNoAHP', {}).get('count', 0) > 0:\n",
        "        feedback_sections.append(\"\"\"\n",
        "CRITICAL: Patient Count Guidelines:\n",
        "- Use only numerical values\n",
        "- For each condition group, specify exact count\n",
        "- Remove words \"patients\" or \"patient\"\n",
        "Example: \"25\" not \"25 patients\\\"\"\"\")\n",
        "\n",
        "    # Add overall reminders\n",
        "    feedback_sections.append(\"\"\"\n",
        "CRITICAL REMINDERS:\n",
        "1. Multiple Conditions:\n",
        "   - Always list ALL conditions, types, or measurements\n",
        "   - Use commas to separate multiple values\n",
        "   - Maintain order correspondence across related fields\n",
        "\n",
        "2. Standardization:\n",
        "   - Do not modify original disease names\n",
        "   - Use \"face turn\" consistently\n",
        "   - Keep all measurement values\n",
        "\n",
        "3. Completeness:\n",
        "   - Include all conditions found in the text\n",
        "   - Do not simplify or combine conditions\n",
        "   - Maintain all original measurements\"\"\")\n",
        "\n",
        "    # Combine base prompt with dynamic feedback\n",
        "    enhanced_prompt = base_prompt + \"\\n\\nADDITIONAL CRITICAL GUIDELINES:\\n\" + \"\\n\".join(feedback_sections)\n",
        "    return enhanced_prompt\n",
        "\n",
        "def analyze_extraction_issues(previous_results: list) -> dict:\n",
        "    \"\"\"Analyzes issues in previous extraction attempts with enhanced pattern recognition\"\"\"\n",
        "    issues = {}\n",
        "\n",
        "    for result in previous_results:\n",
        "        if 'detailed_results' in result:\n",
        "            for field, details in result['detailed_results'].items():\n",
        "                if not details['match']:\n",
        "                    if field not in issues:\n",
        "                        issues[field] = {\n",
        "                            'count': 0,\n",
        "                            'mismatches': [],\n",
        "                            'patterns': set()\n",
        "                        }\n",
        "                    issues[field]['count'] += 1\n",
        "                    issues[field]['mismatches'].append({\n",
        "                        'expected': details['expected'],\n",
        "                        'got': details['got']\n",
        "                    })\n",
        "\n",
        "                    # Analyze specific error patterns\n",
        "                    if field == 'Diagnose':\n",
        "                        if 'retraction' in details['got'] and 'retraction' not in details['expected']:\n",
        "                            issues[field]['patterns'].add('disease_name_modification')\n",
        "                    elif field == 'AHPType':\n",
        "                        if 'head turn' in details['got'] and 'face turn' in details['expected']:\n",
        "                            issues[field]['patterns'].add('turn_terminology')\n",
        "                    elif field in ['DegreePD', 'AHPDegree']:\n",
        "                        if ',' in details['expected'] and ',' not in details['got']:\n",
        "                            issues[field]['patterns'].add('missing_multiple_values')\n",
        "\n",
        "    return issues\n",
        "\n",
        "\n",
        "def save_iteration_metadata(metadata: dict, iterations_results: list, output_path: str):\n",
        "    \"\"\"Save metadata with iteration analysis\"\"\"\n",
        "    try:\n",
        "        output_dir = os.path.dirname(output_path)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Create comprehensive output\n",
        "        output_data = {\n",
        "            \"final_metadata\": metadata,\n",
        "            \"iteration_analysis\": {\n",
        "                \"total_iterations\": len(iterations_results),\n",
        "                \"accuracy_progression\": [r['accuracy'] for r in iterations_results],\n",
        "                \"prompt_evolution\": [r['prompt_used'] for r in iterations_results],\n",
        "                \"field_specific_issues\": analyze_extraction_issues(iterations_results)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(output_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        logging.info(f\"Saved detailed results to {output_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving iteration metadata: {e}\")\n",
        "\n",
        "def analyze_prompt_effectiveness(results: dict) -> dict:\n",
        "    \"\"\"Analyze how different prompt variations performed\"\"\"\n",
        "    prompt_performance = {}\n",
        "\n",
        "    # Track performance for each prompt version\n",
        "    for iteration in results['iterations_results']:\n",
        "        prompt_hash = hash(iteration['prompt_used'])\n",
        "        if prompt_hash not in prompt_performance:\n",
        "            prompt_performance[prompt_hash] = {\n",
        "                'accuracies': [],\n",
        "                'prompt': iteration['prompt_used']\n",
        "            }\n",
        "        prompt_performance[prompt_hash]['accuracies'].append(iteration['accuracy'])\n",
        "\n",
        "    # Calculate metrics for each prompt version\n",
        "    analysis = {}\n",
        "    for prompt_hash, data in prompt_performance.items():\n",
        "        analysis[prompt_hash] = {\n",
        "            'mean_accuracy': np.mean(data['accuracies']),\n",
        "            'std_accuracy': np.std(data['accuracies']),\n",
        "            'usage_count': len(data['accuracies']),\n",
        "            'prompt': data['prompt']\n",
        "        }\n",
        "\n",
        "    return analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "wf16Q7P07Q3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import logging\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler('extraction.log')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Global variables\n",
        "patient_fields = {\n",
        "    'Diagnose': 'Diagnose',\n",
        "    'patientsNoAHP': 'patientsNoAHP',\n",
        "    'AHPType': 'AHPType',\n",
        "    'AHPDirection': 'AHPDirection',\n",
        "    'AHPDegree': 'AHPDegree',\n",
        "    'Eye': 'Eye',\n",
        "    'EyeMisalignment': 'EyeMisalignment',\n",
        "    'DegreePD': 'DegreePD'\n",
        "}\n",
        "\n",
        "def format_number(number):\n",
        "    \"\"\"Format number to handle floating point precision\"\"\"\n",
        "    return f\"{float(number):.1f}\".rstrip('0').rstrip('.')\n",
        "\n",
        "def clean_value(value):\n",
        "    \"\"\"Clean and standardize value for comparison\"\"\"\n",
        "    if pd.isna(value) or value is None or str(value).lower().strip() == 'nan':\n",
        "        return \"\"\n",
        "\n",
        "    value = str(value).lower().strip()\n",
        "\n",
        "    if '<' in value or '>' in value:\n",
        "        try:\n",
        "            num = re.search(r'[<>]\\s*(\\d+\\.?\\d*)', value)\n",
        "            if num:\n",
        "                number = format_number(float(num.group(1)))\n",
        "                symbol = '<' if '<' in value else '>'\n",
        "                return f\"{symbol}{number}°\"\n",
        "        except ValueError:\n",
        "            return value\n",
        "    elif '±' in value:\n",
        "        try:\n",
        "            base, uncertainty = value.split('±')\n",
        "            base_num = format_number(float(base.replace('°', '').strip()))\n",
        "            uncertainty_num = format_number(float(uncertainty.replace('°', '').strip()))\n",
        "            return f\"{base_num}° ± {uncertainty_num}°\"\n",
        "        except ValueError:\n",
        "            return value\n",
        "\n",
        "    value = value.replace(' degrees', '').replace(' pd', '')\n",
        "    value = value.replace(' patient', '').replace('s', '')\n",
        "\n",
        "    try:\n",
        "        float_val = float(value.replace('°', '').strip())\n",
        "        return f\"{format_number(float_val)}°\"\n",
        "    except ValueError:\n",
        "        return value\n",
        "\n",
        "def compare_values(gt_value: str, extracted_value: str, field: str) -> bool:\n",
        "    \"\"\"Compare ground truth and extracted values with field-specific logic.\"\"\"\n",
        "    if pd.isna(gt_value) or pd.isna(extracted_value):\n",
        "        return False\n",
        "\n",
        "    gt_value = str(gt_value).lower().strip()\n",
        "    extracted_value = str(extracted_value).lower().strip()\n",
        "\n",
        "    # Handle AHPType field specially - treat 'face turn' and 'head turn' as equivalent\n",
        "    if field == 'AHPType':\n",
        "        gt_turns = ['face turn', 'head turn']\n",
        "        extracted_turns = ['face turn', 'head turn']\n",
        "\n",
        "        if any(turn in gt_value for turn in gt_turns) and any(turn in extracted_value for turn in extracted_turns):\n",
        "            return True\n",
        "        return gt_value == extracted_value\n",
        "\n",
        "    # Handle degree values\n",
        "    elif field in ['AHPDegree', 'DegreePD']:\n",
        "        # Remove non-numeric characters and compare numbers\n",
        "        gt_num = re.search(r'(\\d+(?:\\.\\d+)?)', gt_value)\n",
        "        ex_num = re.search(r'(\\d+(?:\\.\\d+)?)', extracted_value)\n",
        "\n",
        "        if gt_num and ex_num:\n",
        "            return float(gt_num.group(1)) == float(ex_num.group(1))\n",
        "        return False\n",
        "\n",
        "    # Handle disease names\n",
        "    elif field == 'Diagnose':\n",
        "        gt_clean = standardize_disease_names(gt_value)\n",
        "        ex_clean = standardize_disease_names(extracted_value)\n",
        "        return gt_clean == ex_clean\n",
        "\n",
        "    # Default string comparison\n",
        "    return gt_value == extracted_value\n",
        "\n",
        "def normalize_filename(filename):\n",
        "    \"\"\"Normalize filename for matching\"\"\"\n",
        "    try:\n",
        "        # Try to convert to integer if it's a number\n",
        "        return str(int(float(filename)))\n",
        "    except (ValueError, TypeError):\n",
        "        return str(filename).strip()\n",
        "\n",
        "def validate_ground_truth(ground_truth):\n",
        "    \"\"\"Validate ground truth data structure\"\"\"\n",
        "    required_fields = ['FileName', 'PaperTitle', 'patientsNo']\n",
        "    missing_fields = [field for field in required_fields if field not in ground_truth]\n",
        "\n",
        "    if missing_fields:\n",
        "        logging.warning(f\"Missing required fields in ground truth: {missing_fields}\")\n",
        "        return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "A7dPB66gWNIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_ground_truth(filepath):\n",
        "    \"\"\"Read ground truth data with proper handling of multiple patients\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Split content into patient records\n",
        "        patient_records = content.split('**')\n",
        "\n",
        "        # Process each patient record\n",
        "        patients = []\n",
        "        for record in patient_records:\n",
        "            if not record.strip():\n",
        "                continue\n",
        "\n",
        "            lines = [line.strip() for line in record.split('\\n') if line.strip()]\n",
        "            if not lines:\n",
        "                continue\n",
        "\n",
        "            patient = {\n",
        "                'PaperTitle': lines[0],\n",
        "                'Diagnose': lines[1],\n",
        "                'patientsNo': lines[2],\n",
        "                'patientsNoAHP': lines[3],\n",
        "                'AHPType': lines[4],\n",
        "                'AHPDirection': lines[5],\n",
        "                'AHPDegree': lines[6],\n",
        "                'Eye': lines[7],\n",
        "                'EyeMisalignment': lines[8],\n",
        "                'DegreePD': lines[9]\n",
        "            }\n",
        "            patients.append(patient)\n",
        "\n",
        "        return {\n",
        "            'PaperTitle': patients[0]['PaperTitle'],  # Use first patient's paper title\n",
        "            'patientsNo': patients[0]['patientsNo'],  # Use first patient's total count\n",
        "            'patients': patients\n",
        "        }\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reading ground truth file: {e}\")\n",
        "        return None\n",
        "\n",
        "def find_matching_ground_truth(filename, ground_truth_df):\n",
        "    \"\"\"Find matching ground truth with improved filename handling\"\"\"\n",
        "    try:\n",
        "        # Convert filename to numeric if possible\n",
        "        numeric_filename = int(float(filename))\n",
        "\n",
        "        # Try to find exact match first\n",
        "        matching_row = ground_truth_df[\n",
        "            ground_truth_df['FileName'].apply(\n",
        "                lambda x: int(float(x)) if isinstance(x, (int, float, str)) and str(x).replace('.', '').isdigit() else None\n",
        "            ) == numeric_filename\n",
        "        ]\n",
        "\n",
        "        if not matching_row.empty:\n",
        "            return matching_row.iloc[0].to_dict()\n",
        "\n",
        "        # If no match found, log details\n",
        "        logging.warning(f\"No ground truth found for filename: {filename}\")\n",
        "        logging.debug(\"Available filenames in ground truth:\")\n",
        "        for fname in ground_truth_df['FileName'].tolist():\n",
        "            logging.debug(f\"  {fname}\")\n",
        "        return None\n",
        "\n",
        "    except (ValueError, TypeError):\n",
        "        logging.error(f\"Invalid filename format: {filename}\")\n",
        "        return None\n",
        "\n",
        "def prepare_ground_truth_patients(ground_truth_text: str):\n",
        "    \"\"\"Process ground truth data with multiple patients per paper\"\"\"\n",
        "    patients = []\n",
        "    current_patient = {}\n",
        "\n",
        "    # Define the fields in order they appear\n",
        "    fields = ['PaperTitle', 'Diagnose', 'patientsNo', 'patientsNoAHP',\n",
        "              'AHPType', 'AHPDirection', 'AHPDegree', 'Eye',\n",
        "              'EyeMisalignment', 'DegreePD', 'FileName']\n",
        "\n",
        "    # Split by ** to get individual patients\n",
        "    patient_sections = ground_truth_text.split('**')\n",
        "\n",
        "    for section in patient_sections:\n",
        "        if not section.strip():\n",
        "            continue\n",
        "\n",
        "        # Split into lines and remove empty lines\n",
        "        lines = [line.strip() for line in section.split('\\n') if line.strip()]\n",
        "\n",
        "        if len(lines) >= len(fields):\n",
        "            patient = {}\n",
        "            for field, value in zip(fields, lines):\n",
        "                patient[field] = value\n",
        "            patients.append(patient)\n",
        "\n",
        "    if not patients:\n",
        "        logging.warning(\"No patients found in ground truth data\")\n",
        "        return None\n",
        "\n",
        "    # Group patients by paper (FileName)\n",
        "    papers = {}\n",
        "    for patient in patients:\n",
        "        filename = patient['FileName']\n",
        "        if filename not in papers:\n",
        "            papers[filename] = {\n",
        "                'PaperTitle': patient['PaperTitle'],\n",
        "                'patientsNo': patient['patientsNo'],\n",
        "                'patients': []\n",
        "            }\n",
        "        papers[filename]['patients'].append({\n",
        "            'Diagnose': patient['Diagnose'],\n",
        "            'patientsNoAHP': patient['patientsNoAHP'],\n",
        "            'AHPType': patient['AHPType'],\n",
        "            'AHPDirection': patient['AHPDirection'],\n",
        "            'AHPDegree': patient['AHPDegree'],\n",
        "            'Eye': patient['Eye'],\n",
        "            'EyeMisalignment': patient['EyeMisalignment'],\n",
        "            'DegreePD': patient['DegreePD']\n",
        "        })\n",
        "\n",
        "    return papers\n",
        "\n",
        "\n",
        "def find_best_matching_patient(patient, ground_truth_patients, matched_indices):\n",
        "    \"\"\"Find best matching patient with improved matching logic\"\"\"\n",
        "    best_match_idx = None\n",
        "    best_match_score = 0\n",
        "\n",
        "    weights = {\n",
        "        'Diagnose': 3.0,\n",
        "        'AHPDegree': 2.5,\n",
        "        'DegreePD': 2.5,\n",
        "        'Eye': 2.0,\n",
        "        'AHPType': 2.0,\n",
        "        'AHPDirection': 1.5,\n",
        "        'EyeMisalignment': 1.5,\n",
        "        'patientsNoAHP': 1.0\n",
        "    }\n",
        "\n",
        "    max_possible_score = sum(weights.values())\n",
        "\n",
        "    for idx, gt_patient in enumerate(ground_truth_patients):\n",
        "        if idx in matched_indices:\n",
        "            continue\n",
        "\n",
        "        score = 0\n",
        "        fields_compared = 0\n",
        "\n",
        "        for field, weight in weights.items():\n",
        "            model_value = str(patient.get(field, '')).lower().strip()\n",
        "            gt_value = str(gt_patient.get(field, '')).lower().strip()\n",
        "\n",
        "            if model_value and gt_value:\n",
        "                fields_compared += 1\n",
        "\n",
        "                # For degree values, compare numeric values\n",
        "                if field in ['AHPDegree', 'DegreePD']:\n",
        "                    try:\n",
        "                        model_num = float(re.sub(r'[^\\d.]', '', model_value))\n",
        "                        gt_num = float(re.sub(r'[^\\d.]', '', gt_value))\n",
        "                        # Consider it a match if within 1 degree\n",
        "                        if abs(model_num - gt_num) <= 1:\n",
        "                            score += weight\n",
        "                    except (ValueError, TypeError):\n",
        "                        pass\n",
        "\n",
        "                # For AHPType, handle face/head turn equivalence\n",
        "                elif field == 'AHPType':\n",
        "                    model_norm = model_value.replace('head turn', 'face turn')\n",
        "                    gt_norm = gt_value.replace('head turn', 'face turn')\n",
        "                    if model_norm == gt_norm:\n",
        "                        score += weight\n",
        "\n",
        "                # For other fields, exact match but case-insensitive\n",
        "                elif model_value == gt_value:\n",
        "                    score += weight\n",
        "\n",
        "        # Normalize score and consider it a match if above threshold\n",
        "        if fields_compared > 0:\n",
        "            normalized_score = score / (fields_compared * max(weights.values()))\n",
        "            if normalized_score > best_match_score:\n",
        "                best_match_score = normalized_score\n",
        "                best_match_idx = idx\n",
        "\n",
        "    # Only return a match if score is above threshold\n",
        "    threshold = 0.5  # Adjust this value as needed\n",
        "    if best_match_score < threshold:\n",
        "        return None, 0\n",
        "\n",
        "    return best_match_idx, best_match_score"
      ],
      "metadata": {
        "id": "9UNYZNRPrZRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_output_to_ground_truth(model_output, ground_truth):\n",
        "    \"\"\"Compare extracted metadata with ground truth for all patients.\"\"\"\n",
        "    matches = 0\n",
        "    total = 0\n",
        "    detailed_results = {\n",
        "        'metadata': {},\n",
        "        'patients': []\n",
        "    }\n",
        "\n",
        "    # Standardize outputs\n",
        "    standardized_output = apply_standardization(model_output)\n",
        "    ground_truth_patients = prepare_ground_truth_patients(ground_truth)\n",
        "\n",
        "    logging.info(f\"Comparing {len(standardized_output.get('patients', []))} extracted patients \"\n",
        "                f\"with {len(ground_truth_patients)} ground truth patients\")\n",
        "\n",
        "    # Compare metadata fields\n",
        "    metadata_fields = ['PaperTitle', 'patientsNo']\n",
        "    for field in metadata_fields:\n",
        "        if field in ground_truth:\n",
        "            total += 1\n",
        "            if field in standardized_output:\n",
        "                gt_value = clean_value(ground_truth[field])\n",
        "                model_value = clean_value(standardized_output[field])\n",
        "                matches += (gt_value == model_value)\n",
        "                detailed_results['metadata'][field] = {\n",
        "                    'expected': gt_value,\n",
        "                    'got': model_value,\n",
        "                    'match': gt_value == model_value\n",
        "                }\n",
        "\n",
        "    # Get extracted patients and track matching\n",
        "    extracted_patients = standardized_output.get('patients', [])\n",
        "    matched_gt_indices = set()\n",
        "    matched_ex_indices = set()\n",
        "\n",
        "    # First pass: match patients with highest similarity\n",
        "    for ex_idx, extracted_patient in enumerate(extracted_patients):\n",
        "        best_match_idx = None\n",
        "        best_match_score = 0\n",
        "\n",
        "        for gt_idx, gt_patient in enumerate(ground_truth_patients):\n",
        "            if gt_idx in matched_gt_indices:\n",
        "                continue\n",
        "\n",
        "            match_score = calculate_patient_similarity(extracted_patient, gt_patient)\n",
        "            if match_score > best_match_score:\n",
        "                best_match_score = match_score\n",
        "                best_match_idx = gt_idx\n",
        "\n",
        "        if best_match_score > 0.5:  # Threshold for considering a match\n",
        "            matched_gt_indices.add(best_match_idx)\n",
        "            matched_ex_indices.add(ex_idx)\n",
        "\n",
        "    # Process all patients and record results\n",
        "    for idx, ex_patient in enumerate(extracted_patients):\n",
        "        patient_results = {\n",
        "            'patient_index': idx + 1,\n",
        "            'matches': 0,\n",
        "            'total': 0,\n",
        "            'fields': {},\n",
        "            'has_ground_truth': idx in matched_ex_indices\n",
        "        }\n",
        "\n",
        "        # Compare with matched ground truth patient or mark as extra\n",
        "        if idx in matched_ex_indices:\n",
        "            gt_idx = [i for i in range(len(ground_truth_patients))\n",
        "                     if i in matched_gt_indices and\n",
        "                     calculate_patient_similarity(ex_patient, ground_truth_patients[i]) > 0.5][0]\n",
        "            gt_patient = ground_truth_patients[gt_idx]\n",
        "\n",
        "            # Compare each field\n",
        "            for field in patient_fields:\n",
        "                patient_results['total'] += 1\n",
        "                gt_value = gt_patient.get(field, \"\")\n",
        "                ex_value = ex_patient.get(field, \"\")\n",
        "\n",
        "                is_match = compare_values(gt_value, ex_value, field=field)\n",
        "                patient_results['matches'] += int(is_match)\n",
        "\n",
        "                patient_results['fields'][field] = {\n",
        "                    'expected': str(gt_value),\n",
        "                    'got': str(ex_value),\n",
        "                    'match': is_match,\n",
        "                    'ground_truth_available': True\n",
        "                }\n",
        "        else:\n",
        "            # Record extra extracted patient\n",
        "            for field in patient_fields:\n",
        "                patient_results['fields'][field] = {\n",
        "                    'expected': \"No ground truth data\",\n",
        "                    'got': str(ex_patient.get(field, \"\")),\n",
        "                    'match': False,\n",
        "                    'ground_truth_available': False\n",
        "                }\n",
        "\n",
        "        detailed_results['patients'].append(patient_results)\n",
        "        matches += patient_results['matches']\n",
        "        total += patient_results['total']\n",
        "\n",
        "    accuracy = matches / total if total > 0 else 0\n",
        "    return accuracy, detailed_results\n",
        "\n",
        "def print_detailed_results(detailed_results, matches, total):\n",
        "    \"\"\"Print detailed comparison results\"\"\"\n",
        "    print(\"\\nDetailed Comparison Results:\")\n",
        "\n",
        "    # Print metadata results\n",
        "    print(\"\\nMetadata Fields:\")\n",
        "    for field, result in detailed_results['metadata'].items():\n",
        "        status = \"✓\" if result['match'] else \"✗\"\n",
        "        print(f\"{status} {field}:\")\n",
        "        print(f\"  Expected: {result['expected']}\")\n",
        "        print(f\"  Got: {result['got']}\")\n",
        "\n",
        "    print(\"\\nPatient Information:\")\n",
        "    print(\"-\" * 20)\n",
        "    for patient in detailed_results.get('patients', []):\n",
        "        print(f\"\\nPatient {patient['patient_index']}:\")\n",
        "        if patient['has_ground_truth']:\n",
        "            print(f\"Accuracy: {patient['accuracy']:.2f} ({patient['matches']}/{patient['total']} fields matched)\")\n",
        "        else:\n",
        "            print(\"WARNING: No ground truth data available for comparison\")\n",
        "            print(\"This patient was extracted but cannot be verified\")\n",
        "\n",
        "        for field, field_result in patient['fields'].items():\n",
        "            if field_result['ground_truth_available']:\n",
        "                match_status = \"✓\" if field_result['match'] else \"✗\"\n",
        "                print(f\"\\n{match_status} {field}:\")\n",
        "                print(f\"  Ground Truth: {field_result['expected']}\")\n",
        "                print(f\"  Extracted: {field_result['got']}\")\n",
        "                if not field_result['match']:\n",
        "                    print(\"  -> Field mismatch\")\n",
        "            else:\n",
        "                print(f\"\\n? {field}:\")\n",
        "                print(f\"  Ground Truth: No data available\")\n",
        "                print(f\"  Extracted: {field_result['got']}\")\n",
        "                print(\"  -> Cannot verify - no ground truth data\")\n",
        "\n",
        "    # Print overall accuracy\n",
        "    accuracy = matches / total if total > 0 else 0\n",
        "    print(f\"\\nOverall Accuracy: {accuracy:.2f} ({matches}/{total} fields matched)\")\n",
        "\n",
        "def calculate_patient_similarity(patient1, patient2):\n",
        "    \"\"\"Calculate similarity score between two patients\"\"\"\n",
        "    score = 0\n",
        "    total = 0\n",
        "\n",
        "    # Weights for different fields\n",
        "    weights = {\n",
        "        'Diagnose': 3.0,\n",
        "        'AHPType': 2.0,\n",
        "        'AHPDirection': 1.5,\n",
        "        'AHPDegree': 2.0,\n",
        "        'Eye': 1.5,\n",
        "        'EyeMisalignment': 2.0,\n",
        "        'DegreePD': 2.0,\n",
        "        'patientsNoAHP': 1.0\n",
        "    }\n",
        "\n",
        "    for field, weight in weights.items():\n",
        "        if field in patient1 or field in patient2:\n",
        "            total += weight\n",
        "            val1 = str(patient1.get(field, \"\")).lower().strip()\n",
        "            val2 = str(patient2.get(field, \"\")).lower().strip()\n",
        "\n",
        "            if field in ['AHPDegree', 'DegreePD']:\n",
        "                # Compare numeric values with tolerance\n",
        "                try:\n",
        "                    num1 = float(re.sub(r'[^\\d.]', '', val1))\n",
        "                    num2 = float(re.sub(r'[^\\d.]', '', val2))\n",
        "                    if abs(num1 - num2) <= 2:  # 2-degree tolerance\n",
        "                        score += weight\n",
        "                except ValueError:\n",
        "                    pass\n",
        "            else:\n",
        "                # Direct comparison for other fields\n",
        "                if val1 == val2:\n",
        "                    score += weight\n",
        "                elif field == 'Diagnose' and (val1 in val2 or val2 in val1):\n",
        "                    score += weight * 0.8  # Partial match\n",
        "\n",
        "    return score / total if total > 0 else 0\n",
        "\n",
        "def calculate_patient_statistics(detailed_results):\n",
        "    \"\"\"Calculate statistics across all patients\"\"\"\n",
        "    if not detailed_results.get('patients'):\n",
        "        return None\n",
        "\n",
        "    stats = {\n",
        "        'total_patients': len(detailed_results['patients']),\n",
        "        'matched_patients': len([p for p in detailed_results['patients'] if p['has_match']]),\n",
        "        'average_accuracy': 0,\n",
        "        'best_accuracy': 0,\n",
        "        'worst_accuracy': 1,\n",
        "        'field_accuracy': defaultdict(float)\n",
        "    }\n",
        "\n",
        "    # Calculate per-patient statistics\n",
        "    accuracies = []\n",
        "    field_matches = defaultdict(int)\n",
        "    field_totals = defaultdict(int)\n",
        "\n",
        "    for patient in detailed_results['patients']:\n",
        "        if patient['has_match'] and patient['total'] > 0:\n",
        "            accuracies.append(patient['accuracy'])\n",
        "\n",
        "            for field, result in patient['fields'].items():\n",
        "                field_matches[field] += int(result['match'])\n",
        "                field_totals[field] += 1\n",
        "\n",
        "    if accuracies:\n",
        "        stats['average_accuracy'] = np.mean(accuracies)\n",
        "        stats['best_accuracy'] = max(accuracies)\n",
        "        stats['worst_accuracy'] = min(accuracies)\n",
        "\n",
        "        # Calculate per-field accuracy\n",
        "        for field in field_matches:\n",
        "            if field_totals[field] > 0:\n",
        "                stats['field_accuracy'][field] = field_matches[field] / field_totals[field]\n",
        "\n",
        "    return stats\n",
        "\n",
        "def print_comparison_summary(detailed_results):\n",
        "    \"\"\"Print a summary of the comparison results\"\"\"\n",
        "    stats = calculate_patient_statistics(detailed_results)\n",
        "    if not stats:\n",
        "        print(\"No patient data available for summary\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nComparison Summary:\")\n",
        "    print(f\"Total Extracted Patients: {stats['total_patients']}\")\n",
        "    print(f\"Successfully Matched Patients: {stats['matched_patients']}\")\n",
        "    if stats['matched_patients'] > 0:\n",
        "        print(f\"Average Patient Accuracy: {stats['average_accuracy']:.2f}\")\n",
        "        print(f\"Best Patient Accuracy: {stats['best_accuracy']:.2f}\")\n",
        "        print(f\"Worst Patient Accuracy: {stats['worst_accuracy']:.2f}\")\n",
        "\n",
        "        print(\"\\nField-specific Accuracy:\")\n",
        "        for field, accuracy in sorted(stats['field_accuracy'].items()):\n",
        "            print(f\"{field}: {accuracy:.2f}\")\n",
        "\n",
        "def improve_extraction_accuracy(pdf_text: str) -> dict:\n",
        "    # Address specific patterns that caused low accuracy\n",
        "    common_issues = {\n",
        "        # From Paper 69 (Superior Oblique Palsy with complex measurements)\n",
        "        \"measurement_patterns\": [\n",
        "            r\"(\\d+\\.?\\d*)\\s*±\\s*(\\d+\\.?\\d*)\\s*degrees?\",  # For patterns like \"18.92 ± 7.08 degrees\"\n",
        "            r\"(\\d+\\.?\\d*)\\s*degrees?\\s*(?:for|in)\\s*(\\w+\\s+\\w+)\"  # For contextual measurements\n",
        "        ],\n",
        "        # From Paper 93 (Multiple patient groups with similar conditions)\n",
        "        \"patient_group_patterns\": [\n",
        "            r\"(\\d+)\\s*patients?\\s*with\\s*([\\w\\s]+)\",\n",
        "            r\"group\\s*\\d+\\s*:\\s*(\\d+)\\s*patients?\\s*with\\s*([\\w\\s]+)\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "# Based on papers with multiple patient groups (e.g., Paper 40)\n",
        "def handle_multiple_patient_groups(metadata: dict) -> dict:\n",
        "    \"\"\"Improve handling of papers with multiple patient groups\"\"\"\n",
        "\n",
        "    patient_groups = []\n",
        "    current_group = {}\n",
        "\n",
        "    # Pattern seen in successful extractions (e.g., Paper 63 with accuracy 1.0)\n",
        "    for line in metadata['text'].split('\\n'):\n",
        "        if re.match(r'Group|Cohort|Type \\d+:', line):\n",
        "            if current_group:\n",
        "                patient_groups.append(current_group)\n",
        "            current_group = {'group_type': line}\n",
        "\n",
        "    return {'patient_groups': patient_groups}\n",
        "\n",
        "# Based on successful papers like ID 64 (accuracy 1.0)\n",
        "def standardize_ahp_info(ahp_data: dict) -> dict:\n",
        "    \"\"\"Standardize AHP information based on successful patterns\"\"\"\n",
        "\n",
        "    standardized = {\n",
        "        'AHPType': [],\n",
        "        'AHPDirection': [],\n",
        "        'AHPDegree': []\n",
        "    }\n",
        "\n",
        "    # Patterns from successful extractions\n",
        "    type_mapping = {\n",
        "        'head turn': 'face turn',  # Standardize terminology\n",
        "        'chin-up': 'chin-up',\n",
        "        'chin-down': 'chin-down',\n",
        "        'head tilt': 'head tilt'\n",
        "    }\n",
        "\n",
        "    # Handle multiple AHP types\n",
        "    if isinstance(ahp_data['AHPType'], str):\n",
        "        types = [t.strip() for t in ahp_data['AHPType'].split(',')]\n",
        "        standardized['AHPType'] = [type_mapping.get(t, t) for t in types]\n",
        "\n",
        "    return standardized\n",
        "\n",
        "# Based on perfect accuracy cases (e.g., Papers 91, 64)\n",
        "def enhance_disease_standardization(disease_name: str) -> str:\n",
        "    \"\"\"Enhanced disease name standardization based on successful cases\"\"\"\n",
        "\n",
        "    standard_names = {\n",
        "        'duane retraction syndrome': 'Duane syndrome',\n",
        "        'congenital nystagmus': 'Nystagmus',\n",
        "        'superior oblique palsy': 'Superior oblique palsy'\n",
        "    }\n",
        "\n",
        "    # Remove common variations that caused mismatches\n",
        "    disease_name = re.sub(r'\\s*\\([^)]*\\)', '', disease_name)  # Remove parenthetical\n",
        "    disease_name = re.sub(r'\\s*type\\s+[IiVv\\d]+', '', disease_name)  # Remove type specifications\n",
        "\n",
        "    return standard_names.get(disease_name.lower(), disease_name)\n",
        "\n",
        "# Based on successful extractions (e.g., Paper 76)\n",
        "def standardize_measurements(value: str) -> str:\n",
        "    \"\"\"Standardize measurements based on successful patterns\"\"\"\n",
        "\n",
        "    # Handle patterns seen in high-accuracy results\n",
        "    patterns = {\n",
        "        'degree': r'(\\d+(?:\\.\\d+)?)\\s*(?:°|degrees?)',\n",
        "        'range': r'(\\d+)(?:\\s*-\\s*|\\s+to\\s+)(\\d+)\\s*(?:°|degrees?)',\n",
        "        'pd': r'(\\d+(?:\\.\\d+)?)\\s*(?:pd|PD|prism diopters?)'\n",
        "    }\n",
        "\n",
        "    for pattern_type, pattern in patterns.items():\n",
        "        match = re.search(pattern, value)\n",
        "        if match:\n",
        "            if pattern_type == 'range':\n",
        "                return f\"{match.group(1)}-{match.group(2)}°\"\n",
        "            else:\n",
        "                return f\"{match.group(1)}°\"\n",
        "\n",
        "    return value\n",
        "\n",
        "  # Based on papers with multiple conditions (e.g., Paper 40)\n",
        "def handle_complex_cases(metadata: dict) -> dict:\n",
        "    \"\"\"Handle complex cases with multiple conditions/measurements\"\"\"\n",
        "\n",
        "    # Patterns from successful complex extractions\n",
        "    if 'eyeMisalignment' in metadata and 'degree' in metadata:\n",
        "        misalignments = metadata['eyeMisalignment'].split(',')\n",
        "        degrees = metadata['degree'].split(',')\n",
        "\n",
        "        # Ensure one-to-one correspondence\n",
        "        if len(misalignments) == len(degrees):\n",
        "            metadata['aligned_data'] = [\n",
        "                {'misalignment': m.strip(), 'degree': d.strip()}\n",
        "                for m, d in zip(misalignments, degrees)\n",
        "            ]\n",
        "\n",
        "    return metadata"
      ],
      "metadata": {
        "id": "uikkDHyTUFkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_paper_with_improvements(pdf_path: str, prompt_path: str, ground_truth_df: pd.DataFrame) -> dict:\n",
        "    \"\"\"Enhanced paper processing with improvements\"\"\"\n",
        "    max_attempts = 3\n",
        "    min_accuracy = 0.8\n",
        "\n",
        "    try:\n",
        "        # Extract text with improved error handling\n",
        "        pdf_text = extract_text_with_backup(pdf_path)\n",
        "        base_prompt = read_prompt(prompt_path)\n",
        "        filename = Path(pdf_path).stem\n",
        "\n",
        "        # Get ground truth\n",
        "        ground_truth = find_matching_ground_truth(filename, ground_truth_df)\n",
        "        if ground_truth is None:\n",
        "            logging.warning(f\"Skipping {filename} - no ground truth found\")\n",
        "            return None\n",
        "\n",
        "        # Process with multiple attempts and improvements\n",
        "        all_iterations_results = []\n",
        "        best_result = None\n",
        "        best_accuracy = 0\n",
        "\n",
        "        for attempt in range(max_attempts):\n",
        "            logging.info(f\"\\nAttempt {attempt + 1}/{max_attempts} for {filename}\")\n",
        "\n",
        "            # Create improved dynamic prompt\n",
        "            current_prompt = create_improved_prompt(\n",
        "                base_prompt,\n",
        "                all_iterations_results if all_iterations_results else None\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                # Extract with enhanced validation\n",
        "                model_output = ask_question(pdf_text, current_prompt)\n",
        "                metadata = clean_json_output(model_output)\n",
        "\n",
        "                if not metadata:\n",
        "                    continue\n",
        "\n",
        "                # Apply enhanced standardization\n",
        "                metadata = enhance_standardization(metadata)\n",
        "\n",
        "                # Handle complex cases\n",
        "                metadata = handle_complex_cases(metadata)\n",
        "\n",
        "                # Validate extraction\n",
        "                is_valid, errors = validate_extraction(metadata)\n",
        "                if not is_valid:\n",
        "                    logging.warning(f\"Validation errors: {errors}\")\n",
        "                    continue\n",
        "\n",
        "                metadata[\"FileName\"] = filename\n",
        "\n",
        "                # Compare with ground truth using improved comparison\n",
        "                accuracy, detailed_results = compare_output_to_ground_truth_improved(\n",
        "                    metadata,\n",
        "                    ground_truth\n",
        "                )\n",
        "\n",
        "                current_result = {\n",
        "                    'metadata': metadata,\n",
        "                    'accuracy': accuracy,\n",
        "                    'detailed_results': detailed_results,\n",
        "                    'prompt_used': current_prompt,\n",
        "                    'attempt': attempt + 1\n",
        "                }\n",
        "\n",
        "                all_iterations_results.append(current_result)\n",
        "                logging.info(f\"Attempt {attempt + 1} accuracy: {accuracy:.2f}\")\n",
        "\n",
        "                # Update best result if current is better\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_result = {\n",
        "                        'metadata': metadata,\n",
        "                        'iterations_results': all_iterations_results,\n",
        "                        'mean_accuracy': accuracy,\n",
        "                        'prompt_evolution': current_prompt,\n",
        "                        'final_detailed_results': detailed_results\n",
        "                    }\n",
        "\n",
        "                # Early stopping if accuracy is high enough\n",
        "                if accuracy >= min_accuracy:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in extraction attempt {attempt + 1}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return best_result\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {pdf_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_text_with_backup(pdf_path: str) -> str:\n",
        "    \"\"\"Extract text with backup methods\"\"\"\n",
        "    text = \"\"\n",
        "\n",
        "    # Try primary method\n",
        "    try:\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        if text.strip():\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Primary extraction failed: {e}\")\n",
        "\n",
        "    # Try backup methods\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        text = ' '.join(page.extract_text() for page in reader.pages)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"All extraction methods failed: {e}\")\n",
        "        raise\n",
        "\n",
        "    return text\n",
        "\n",
        "def enhance_standardization(metadata: dict) -> dict:\n",
        "    \"\"\"Apply all enhanced standardization\"\"\"\n",
        "    try:\n",
        "        # Standardize disease names\n",
        "        if 'Diagnose' in metadata:\n",
        "            metadata['Diagnose'] = enhance_disease_standardization(metadata['Diagnose'])\n",
        "\n",
        "        # Standardize measurements\n",
        "        for field in ['AHPDegree', 'degree']:\n",
        "            if field in metadata:\n",
        "                metadata[field] = standardize_measurements(metadata[field])\n",
        "\n",
        "        # Standardize AHP information\n",
        "        ahp_fields = {k: v for k, v in metadata.items() if k.startswith('AHP')}\n",
        "        if ahp_fields:\n",
        "            standardized_ahp = standardize_ahp_info(ahp_fields)\n",
        "            metadata.update(standardized_ahp)\n",
        "\n",
        "        # Handle multiple patient groups\n",
        "        if 'patientsINFO' in metadata:\n",
        "            metadata['patientsINFO'] = handle_multiple_patient_groups(metadata)\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Standardization error: {e}\")\n",
        "        return metadata\n",
        "\n",
        "\n",
        "def compare_output_to_ground_truth(model_output, ground_truth):\n",
        "    \"\"\"Compare extracted metadata with ground truth for all patients.\"\"\"\n",
        "    matches = 0\n",
        "    total = 0\n",
        "    detailed_results = {\n",
        "        'metadata': {},\n",
        "        'patients': []\n",
        "    }\n",
        "\n",
        "    # Standardize outputs\n",
        "    standardized_output = apply_standardization(model_output)\n",
        "    ground_truth_patients = prepare_ground_truth_patients(ground_truth)\n",
        "    extracted_patients = standardized_output.get('patients', [])\n",
        "\n",
        "    # The comparison assumes direct index matching, which is problematic\n",
        "    for idx, ex_patient in enumerate(extracted_patients):\n",
        "        # Get ground truth patient if available\n",
        "        gt_patient = ground_truth_patients[idx] if idx < len(ground_truth_patients) else None\n",
        "\n",
        "    # First, log what we're comparing\n",
        "    print(f\"\\nComparison Details:\")\n",
        "    print(f\"Found {len(standardized_output.get('patients', []))} extracted patients\")\n",
        "    print(f\"Found {len(ground_truth_patients)} ground truth patients\")\n",
        "\n",
        "    # Compare metadata fields\n",
        "    metadata_fields = ['PaperTitle', 'patientsNo']\n",
        "    for field in metadata_fields:\n",
        "        if field in ground_truth:\n",
        "            total += 1\n",
        "            if field in standardized_output:\n",
        "                gt_value = clean_value(ground_truth[field])\n",
        "                model_value = clean_value(standardized_output[field])\n",
        "                matches += (gt_value == model_value)\n",
        "                detailed_results['metadata'][field] = {\n",
        "                    'expected': gt_value,\n",
        "                    'got': model_value,\n",
        "                    'match': gt_value == model_value\n",
        "                }\n",
        "\n",
        "    # Compare each patient\n",
        "    extracted_patients = standardized_output.get('patients', [])\n",
        "    for idx, ex_patient in enumerate(extracted_patients):\n",
        "        patient_results = {}\n",
        "        patient_matches = 0\n",
        "        patient_total = 0\n",
        "\n",
        "        # Get ground truth patient if available\n",
        "        gt_patient = ground_truth_patients[idx] if idx < len(ground_truth_patients) else None\n",
        "\n",
        "        if gt_patient:\n",
        "            # Compare each field for existing ground truth\n",
        "            for field in patient_fields.keys():\n",
        "                patient_total += 1\n",
        "                gt_value = gt_patient.get(field, \"\")\n",
        "                ex_value = ex_patient.get(field, \"\")\n",
        "\n",
        "                is_match = compare_values(gt_value, ex_value, field=field)\n",
        "                patient_matches += int(is_match)\n",
        "\n",
        "                patient_results[field] = {\n",
        "                    'expected': str(gt_value),\n",
        "                    'got': str(ex_value),\n",
        "                    'match': is_match,\n",
        "                    'ground_truth_available': True\n",
        "                }\n",
        "        else:\n",
        "            # Mark as extra extracted patient\n",
        "            for field, value in ex_patient.items():\n",
        "                patient_results[field] = {\n",
        "                    'expected': \"No ground truth data\",\n",
        "                    'got': str(value),\n",
        "                    'match': False,\n",
        "                    'ground_truth_available': False\n",
        "                }\n",
        "            patient_total = len(ex_patient)\n",
        "\n",
        "        detailed_results['patients'].append({\n",
        "            'patient_index': idx + 1,\n",
        "            'matches': patient_matches,\n",
        "            'total': patient_total,\n",
        "            'accuracy': patient_matches / patient_total if patient_total > 0 else 0,\n",
        "            'fields': patient_results,\n",
        "            'has_ground_truth': gt_patient is not None\n",
        "        })\n",
        "\n",
        "        matches += patient_matches\n",
        "        total += patient_total\n",
        "\n",
        "    accuracy = matches / total if total > 0 else 0\n",
        "    return accuracy, detailed_results"
      ],
      "metadata": {
        "id": "3Oh9MUdC_Vkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_patient_count(count_str):\n",
        "    \"\"\"Clean and convert patient count strings to numbers\"\"\"\n",
        "    if not count_str:\n",
        "        return 1\n",
        "\n",
        "    try:\n",
        "        # If it's already a number\n",
        "        if isinstance(count_str, (int, float)):\n",
        "            return int(count_str)\n",
        "\n",
        "        # Clean the string\n",
        "        count_str = str(count_str).lower().strip()\n",
        "        # Remove 'patient(s)' and any other text\n",
        "        count_str = re.sub(r'\\s*patients?\\s*', '', count_str)\n",
        "        count_str = re.sub(r'[^\\d.]', '', count_str)\n",
        "\n",
        "        if count_str:\n",
        "            return int(float(count_str))\n",
        "        return 1\n",
        "    except (ValueError, TypeError):\n",
        "        logging.warning(f\"Could not parse patient count: {count_str}, using 1 as default\")\n",
        "        return 1\n",
        "\n",
        "def process_single_paper_with_feedback(pdf_path: str, prompt_path: str, ground_truth_data: dict):\n",
        "    \"\"\"Process paper with feedback system\"\"\"\n",
        "    max_attempts = 3\n",
        "    min_accuracy = 0.8\n",
        "\n",
        "    try:\n",
        "        # Initial setup\n",
        "        pdf_text = extract_text_from_pdf(pdf_path)\n",
        "        base_prompt = read_prompt(prompt_path)\n",
        "        filename = Path(pdf_path).stem\n",
        "\n",
        "        # Parse ground truth data\n",
        "        if not ground_truth_data:\n",
        "            logging.warning(f\"No valid ground truth data found for {filename}\")\n",
        "            return None\n",
        "\n",
        "        # Convert ground truth data into a structure with paper info and patients\n",
        "        ground_truth = {\n",
        "            'PaperTitle': ground_truth_data['PaperTitle'],\n",
        "            'patientsNo': ground_truth_data['patientsNo'],\n",
        "            'patients': []\n",
        "        }\n",
        "\n",
        "        # Process each row and expand based on patientsNoAHP\n",
        "        for _, row in enumerate(ground_truth_data['patients']):\n",
        "            # Get the number of patients for this condition\n",
        "            patient_count = clean_patient_count(row['patientsNoAHP'])\n",
        "\n",
        "            # Create that many patient entries with the same characteristics\n",
        "            for _ in range(patient_count):\n",
        "                patient = {\n",
        "                    'Diagnose': row['Diagnose'],\n",
        "                    'patientsNoAHP': str(patient_count),  # Store as string to match format\n",
        "                    'AHPType': row['AHPType'],\n",
        "                    'AHPDirection': row['AHPDirection'],\n",
        "                    'AHPDegree': row['AHPDegree'],\n",
        "                    'Eye': row['Eye'],\n",
        "                    'EyeMisalignment': row['EyeMisalignment'],\n",
        "                    'DegreePD': row['DegreePD']\n",
        "                }\n",
        "                ground_truth['patients'].append(patient)\n",
        "\n",
        "        total_patients = sum(clean_patient_count(row['patientsNoAHP'])\n",
        "                           for row in ground_truth_data['patients'])\n",
        "\n",
        "        logging.info(f\"Processing paper {filename}\")\n",
        "        logging.info(f\"Ground truth contains {len(ground_truth_data['patients'])} unique condition groups\")\n",
        "        logging.info(f\"Total patients across all groups: {total_patients}\")\n",
        "\n",
        "        # Process with feedback\n",
        "        all_iterations_results = []\n",
        "        best_result = None\n",
        "        best_accuracy = 0\n",
        "\n",
        "        for attempt in range(max_attempts):\n",
        "            logging.info(f\"\\nAttempt {attempt + 1}/{max_attempts} for {filename}\")\n",
        "\n",
        "            # Create dynamic prompt based on previous results\n",
        "            current_prompt = create_dynamic_prompt(\n",
        "                base_prompt,\n",
        "                all_iterations_results if all_iterations_results else None\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                model_output = ask_question(pdf_text, current_prompt)\n",
        "                metadata = clean_json_output(model_output)\n",
        "\n",
        "                if not metadata:\n",
        "                    logging.error(f\"Failed to extract valid metadata in attempt {attempt + 1}\")\n",
        "                    logging.debug(f\"Raw output: {model_output}\")\n",
        "                    continue\n",
        "\n",
        "                metadata[\"FileName\"] = filename\n",
        "\n",
        "                # Verify the total patient count matches\n",
        "                if 'patientsINFO' in metadata:\n",
        "                    extracted_total = sum(clean_patient_count(p.get('patientsNoAHP', 1))\n",
        "                                       for p in metadata['patientsINFO'])\n",
        "                    if extracted_total != total_patients:\n",
        "                        logging.warning(f\"Patient count mismatch: extracted {extracted_total}, \"\n",
        "                                      f\"expected {total_patients}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in extraction attempt {attempt + 1}: {e}\")\n",
        "                logging.exception(\"Detailed error: \")  # This will print the full traceback\n",
        "                continue\n",
        "\n",
        "            # Compare with ground truth\n",
        "            try:\n",
        "                accuracy, detailed_results = compare_output_to_ground_truth(metadata, ground_truth)\n",
        "\n",
        "                current_result = {\n",
        "                    'metadata': metadata,\n",
        "                    'accuracy': accuracy,\n",
        "                    'detailed_results': detailed_results,\n",
        "                    'prompt_used': current_prompt,\n",
        "                    'attempt': attempt + 1\n",
        "                }\n",
        "\n",
        "                all_iterations_results.append(current_result)\n",
        "                logging.info(f\"Attempt {attempt + 1} accuracy: {accuracy:.2f}\")\n",
        "\n",
        "                # Update best result if current is better\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_result = {\n",
        "                        'metadata': metadata,\n",
        "                        'iterations_results': all_iterations_results,\n",
        "                        'mean_accuracy': accuracy,\n",
        "                        'prompt_evolution': current_prompt,\n",
        "                        'final_detailed_results': detailed_results\n",
        "                    }\n",
        "\n",
        "                # Early stopping if accuracy is high enough\n",
        "                if accuracy >= min_accuracy:\n",
        "                    logging.info(f\"Reached minimum accuracy threshold: {accuracy:.2f}\")\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in comparison step: {e}\")\n",
        "                continue\n",
        "\n",
        "        if best_accuracy < min_accuracy:\n",
        "            logging.warning(f\"Failed to reach minimum accuracy threshold. Best accuracy: {best_accuracy:.2f}\")\n",
        "\n",
        "        return best_result\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {pdf_path}: {e}\")\n",
        "        logging.debug(\"Detailed error: \", exc_info=True)\n",
        "        return None\n",
        "\n",
        "def analyze_extraction_issues(results):\n",
        "    \"\"\"Analyze common issues in extraction results\"\"\"\n",
        "    issues = defaultdict(lambda: {'count': 0, 'examples': []})\n",
        "\n",
        "    for result in results:\n",
        "        if 'detailed_results' in result:\n",
        "            # Analyze patient matching issues\n",
        "            for patient in result['detailed_results'].get('patients', []):\n",
        "                if not patient.get('has_match'):\n",
        "                    issues['unmatched_patients']['count'] += 1\n",
        "\n",
        "                for field, field_result in patient.get('fields', {}).items():\n",
        "                    if not field_result.get('match'):\n",
        "                        issues[f'{field}_mismatch']['count'] += 1\n",
        "                        issues[f'{field}_mismatch']['examples'].append({\n",
        "                            'expected': field_result['expected'],\n",
        "                            'got': field_result['got']\n",
        "                        })\n",
        "\n",
        "    return issues\n",
        "\n",
        "def create_dynamic_prompt(base_prompt: str, previous_results: list = None) -> str:\n",
        "    \"\"\"Creates a dynamic prompt that adapts based on previous extraction attempts\"\"\"\n",
        "    if not previous_results:\n",
        "        return base_prompt\n",
        "\n",
        "    issues = analyze_extraction_issues(previous_results)\n",
        "    feedback_sections = []\n",
        "\n",
        "    # Add feedback based on common issues\n",
        "    if issues['unmatched_patients']['count'] > 0:\n",
        "        feedback_sections.append(\"\"\"\n",
        "Important: Patient Matching Guidelines\n",
        "- Extract information for each distinct patient separately\n",
        "- Maintain consistency in patient ordering\n",
        "- Include all relevant fields for each patient\"\"\")\n",
        "\n",
        "    for field in patient_fields.keys():\n",
        "        if issues.get(f'{field}_mismatch', {}).get('count', 0) > 0:\n",
        "            examples = issues[f'{field}_mismatch']['examples'][:3]  # Show up to 3 examples\n",
        "            feedback_sections.append(f\"\"\"\n",
        "{field} Guidelines:\n",
        "- Current accuracy is low for this field\n",
        "- Examples of mismatches:\n",
        "{chr(10).join(f'  Expected: {ex[\"expected\"]}, Got: {ex[\"got\"]}' for ex in examples)}\"\"\")\n",
        "\n",
        "    return base_prompt + \"\\n\\nBased on previous attempts, please pay special attention to:\\n\" + \"\\n\".join(feedback_sections)\n"
      ],
      "metadata": {
        "id": "-Qkn4DY2UFhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wINTKKFpWNDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Js7MWalAWNA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Tuple\n",
        "\n",
        "\n",
        "def convert_json_to_standardized_records(json_data: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Convert JSON output to list of standardized records.\"\"\"\n",
        "    patients_info = json_data.get(\"patientsINFO\", [])\n",
        "    if not isinstance(patients_info, list):\n",
        "        patients_info = [patients_info]\n",
        "\n",
        "    standardized_records = []\n",
        "    filename = normalize_filename(json_data.get(\"FileName\", \"\"))\n",
        "\n",
        "    for patient in patients_info:\n",
        "        record = {\n",
        "            \"FileName\": filename,\n",
        "            \"PaperTitle\": json_data.get(\"PaperTitle\", \"\"),\n",
        "            \"patientsNo\": json_data.get(\"patientsNo\", \"\"),\n",
        "            \"Diagnose\": standardize_disease_names(patient.get(\"Diagnose\", \"\")),\n",
        "            \"patientsNoAHP\": patient.get(\"patientsNoAHP\", \"\"),\n",
        "            \"AHPType\": patient.get(\"AHPType\", \"\").lower().strip(),\n",
        "            \"AHPDirection\": patient.get(\"AHPDirection\", \"\").lower().strip(),\n",
        "            \"AHPDegree\": standardize_measurements(patient.get(\"AHPDegree\", \"\")),\n",
        "            \"Eye\": patient.get(\"eye\", \"\").lower().strip(),\n",
        "            \"EyeMisalignment\": patient.get(\"eyeMisalignment\", \"\").lower().strip(),\n",
        "            \"DegreePD\": standardize_measurements(patient.get(\"degree\", \"\"))\n",
        "        }\n",
        "        standardized_records.append(record)\n",
        "\n",
        "    return standardized_records\n",
        "\n",
        "def compare_field_values(extracted_value: Any, ground_truth_value: Any, field: str) -> bool:\n",
        "    \"\"\"Compare values with field-specific rules.\"\"\"\n",
        "    if pd.isna(extracted_value) and pd.isna(ground_truth_value):\n",
        "        return True\n",
        "    if pd.isna(extracted_value) or pd.isna(ground_truth_value):\n",
        "        return False\n",
        "\n",
        "    ext_val = str(extracted_value).lower().strip()\n",
        "    gt_val = str(ground_truth_value).lower().strip()\n",
        "\n",
        "    # Field-specific comparisons\n",
        "    if field == 'AHPType':\n",
        "        # Treat 'face turn' and 'head turn' as equivalent\n",
        "        turn_variations = ['face turn', 'head turn']\n",
        "        if any(turn in ext_val for turn in turn_variations) and any(turn in gt_val for turn in turn_variations):\n",
        "            return True\n",
        "        return ext_val == gt_val\n",
        "\n",
        "    elif field in ['AHPDegree', 'DegreePD']:\n",
        "        # Compare numeric values from degree measurements\n",
        "        ext_num = re.search(r'(\\d+(?:\\.\\d+)?)', ext_val)\n",
        "        gt_num = re.search(r'(\\d+(?:\\.\\d+)?)', gt_val)\n",
        "        if ext_num and gt_num:\n",
        "            return float(ext_num.group(1)) == float(gt_num.group(1))\n",
        "        return False\n",
        "\n",
        "    elif field == 'Diagnose':\n",
        "        # Standardize disease names for comparison\n",
        "        ext_clean = standardize_disease_names(ext_val)\n",
        "        gt_clean = standardize_disease_names(gt_val)\n",
        "        return ext_clean == gt_clean\n",
        "\n",
        "    elif field == 'patientsNo' or field == 'patientsNoAHP':\n",
        "        # Compare numeric values only\n",
        "        ext_num = re.search(r'\\d+', ext_val)\n",
        "        gt_num = re.search(r'\\d+', gt_val)\n",
        "        if ext_num and gt_num:\n",
        "            return ext_num.group() == gt_num.group()\n",
        "        return False\n",
        "\n",
        "    # Default string comparison\n",
        "    return ext_val == gt_val\n",
        "\n",
        "def compare_with_ground_truth(\n",
        "    extracted_records: List[Dict[str, Any]],\n",
        "    ground_truth_df: pd.DataFrame\n",
        ") -> Tuple[Dict[str, Any], pd.DataFrame]:\n",
        "    \"\"\"Compare extracted records with ground truth, matching by filename.\"\"\"\n",
        "    # Convert extracted records to DataFrame\n",
        "    extracted_df = pd.DataFrame(extracted_records)\n",
        "\n",
        "    # Normalize filenames in ground truth\n",
        "    ground_truth_df['FileName'] = ground_truth_df['FileName'].apply(normalize_filename)\n",
        "\n",
        "    comparison_results = {\n",
        "        \"total_files\": len(extracted_df['FileName'].unique()),\n",
        "        \"matched_files\": 0,\n",
        "        \"field_accuracy\": {},\n",
        "        \"file_results\": {},\n",
        "        \"overall_accuracy\": 0.0\n",
        "    }\n",
        "\n",
        "    matched_records = []\n",
        "\n",
        "    # Compare each file\n",
        "    for filename in extracted_df['FileName'].unique():\n",
        "        extracted_file_records = extracted_df[extracted_df['FileName'] == filename]\n",
        "        ground_truth_file_records = ground_truth_df[ground_truth_df['FileName'] == filename]\n",
        "\n",
        "        if ground_truth_file_records.empty:\n",
        "            comparison_results[\"file_results\"][filename] = {\n",
        "                \"status\": \"no_ground_truth\",\n",
        "                \"accuracy\": 0.0,\n",
        "                \"details\": \"No matching ground truth found\"\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        comparison_results[\"matched_files\"] += 1\n",
        "        file_results = {\n",
        "            \"status\": \"matched\",\n",
        "            \"field_matches\": {},\n",
        "            \"total_matches\": 0,\n",
        "            \"total_fields\": 0\n",
        "        }\n",
        "\n",
        "        # Compare fields\n",
        "        for field in extracted_file_records.columns:\n",
        "            if field == 'FileName':\n",
        "                continue\n",
        "\n",
        "            ext_values = extracted_file_records[field].tolist()\n",
        "            gt_values = ground_truth_file_records[field].tolist()\n",
        "\n",
        "            # Handle different numbers of records\n",
        "            max_len = max(len(ext_values), len(gt_values))\n",
        "            ext_values.extend([''] * (max_len - len(ext_values)))\n",
        "            gt_values.extend([''] * (max_len - len(gt_values)))\n",
        "\n",
        "            # Compare values using field-specific comparison\n",
        "            matches = sum(1 for e, g in zip(ext_values, gt_values)\n",
        "                        if compare_field_values(e, g, field))\n",
        "\n",
        "            file_results[\"field_matches\"][field] = {\n",
        "                \"matches\": matches,\n",
        "                \"total\": max_len,\n",
        "                \"accuracy\": matches / max_len if max_len > 0 else 0\n",
        "            }\n",
        "\n",
        "            file_results[\"total_matches\"] += matches\n",
        "            file_results[\"total_fields\"] += max_len\n",
        "\n",
        "        file_results[\"accuracy\"] = (\n",
        "            file_results[\"total_matches\"] / file_results[\"total_fields\"]\n",
        "            if file_results[\"total_fields\"] > 0 else 0\n",
        "        )\n",
        "\n",
        "        comparison_results[\"file_results\"][filename] = file_results\n",
        "\n",
        "        # Add matched records to output DataFrame\n",
        "        matched_records.extend([{\n",
        "            **record,\n",
        "            \"ground_truth_match\": True,\n",
        "            \"accuracy\": file_results[\"accuracy\"]\n",
        "        } for _, record in extracted_file_records.iterrows()])\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    total_matches = sum(r[\"total_matches\"]\n",
        "                       for r in comparison_results[\"file_results\"].values()\n",
        "                       if r[\"status\"] == \"matched\")\n",
        "    total_fields = sum(r[\"total_fields\"]\n",
        "                      for r in comparison_results[\"file_results\"].values()\n",
        "                      if r[\"status\"] == \"matched\")\n",
        "\n",
        "    comparison_results[\"overall_accuracy\"] = (\n",
        "        total_matches / total_fields if total_fields > 0 else 0\n",
        "    )\n",
        "\n",
        "    return comparison_results, pd.DataFrame(matched_records)\n",
        "\n",
        "def process_and_compare(json_file_path: str, ground_truth_path: str) -> Tuple[Dict[str, Any], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Process JSON file and compare with ground truth.\n",
        "    Returns comparison results and matched records.\n",
        "    \"\"\"\n",
        "    # Load JSON data\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    # Load ground truth\n",
        "    ground_truth_df = pd.read_csv(ground_truth_path)\n",
        "\n",
        "    # Convert JSON to standardized records\n",
        "    standardized_records = convert_json_to_standardized_records(json_data)\n",
        "\n",
        "    # Compare with ground truth\n",
        "    results, matched_df = compare_with_ground_truth(standardized_records, ground_truth_df)\n",
        "\n",
        "    return results, matched_df\n",
        "\n",
        "def print_comparison_results(results: Dict[str, Any]) -> None:\n",
        "    \"\"\"Print formatted comparison results.\"\"\"\n",
        "    print(\"\\nComparison Results:\")\n",
        "    print(f\"Total Files: {results['total_files']}\")\n",
        "    print(f\"Matched Files: {results['matched_files']}\")\n",
        "    print(f\"Overall Accuracy: {results['overall_accuracy']:.2%}\")\n",
        "\n",
        "    print(\"\\nFile-level Results:\")\n",
        "    for filename, file_results in results['file_results'].items():\n",
        "        print(f\"\\nFile: {filename}\")\n",
        "        if file_results['status'] == 'matched':\n",
        "            print(f\"Accuracy: {file_results['accuracy']:.2%}\")\n",
        "            print(\"Field-level Accuracy:\")\n",
        "            for field, field_result in file_results['field_matches'].items():\n",
        "                print(f\"  {field}: {field_result['accuracy']:.2%} \"\n",
        "                      f\"({field_result['matches']}/{field_result['total']})\")\n",
        "        else:\n",
        "            print(f\"Status: {file_results['status']}\")\n",
        "            print(f\"Details: {file_results['details']}\")"
      ],
      "metadata": {
        "id": "x5vLfEp7WM-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_with_ground_truth(\n",
        "    extracted_records: List[Dict[str, Any]],\n",
        "    ground_truth_df: pd.DataFrame\n",
        ") -> Tuple[Dict[str, Any], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Compare extracted records with ground truth, matching by filename.\n",
        "    Returns comparison results and matched records DataFrame.\n",
        "    \"\"\"\n",
        "    # Create a copy of ground truth DataFrame\n",
        "    ground_truth = ground_truth_df.copy()\n",
        "\n",
        "    # Convert extracted records to DataFrame\n",
        "    extracted_df = pd.DataFrame(extracted_records)\n",
        "\n",
        "    # Validate filename existence\n",
        "    if 'FileName' not in extracted_df.columns:\n",
        "        logging.error(\"FileName not found in extracted records\")\n",
        "        return None, pd.DataFrame()\n",
        "\n",
        "    if 'FileName' not in ground_truth.columns:\n",
        "        logging.error(\"FileName not found in ground truth data\")\n",
        "        return None, pd.DataFrame()\n",
        "\n",
        "    # Ensure we have matching data to compare\n",
        "    if not set(extracted_df['FileName'].unique()) & set(ground_truth['FileName'].unique()):\n",
        "        logging.error(\"No matching filenames found between extracted data and ground truth\")\n",
        "        return None, pd.DataFrame()\n",
        "\n",
        "    comparison_results = {\n",
        "        \"total_fields_compared\": 0,\n",
        "        \"total_matches\": 0,\n",
        "        \"field_accuracy\": {},\n",
        "        \"patient_matches\": [],\n",
        "        \"overall_accuracy\": 0.0\n",
        "    }\n",
        "\n",
        "    # Define fields to compare\n",
        "    fields_to_compare = [\n",
        "        'PaperTitle', 'patientsNo', 'Diagnose', 'patientsNoAHP',\n",
        "        'AHPType', 'AHPDirection', 'AHPDegree', 'Eye',\n",
        "        'EyeMisalignment', 'DegreePD'\n",
        "    ]\n",
        "\n",
        "    # Compare only records with matching filenames\n",
        "    for filename in extracted_df['FileName'].unique():\n",
        "        if filename not in ground_truth['FileName'].values:\n",
        "            logging.warning(f\"Skipping comparison for file {filename} - no ground truth data\")\n",
        "            continue\n",
        "\n",
        "        ext_file_records = extracted_df[extracted_df['FileName'] == filename]\n",
        "        gt_file_records = ground_truth[ground_truth['FileName'] == filename]\n",
        "\n",
        "        # Compare each field for this file\n",
        "        for field in fields_to_compare:\n",
        "            if field not in ext_file_records.columns or field not in gt_file_records.columns:\n",
        "                continue\n",
        "\n",
        "            gt_values = gt_file_records[field].fillna('').astype(str)\n",
        "            ex_values = ext_file_records[field].fillna('').astype(str)\n",
        "\n",
        "            # Handle different lengths\n",
        "            max_len = max(len(gt_values), len(ex_values))\n",
        "            gt_values = gt_values.tolist()\n",
        "            ex_values = ex_values.tolist()\n",
        "\n",
        "            # Pad shorter list with empty strings\n",
        "            gt_values.extend([''] * (max_len - len(gt_values)))\n",
        "            ex_values.extend([''] * (max_len - len(ex_values)))\n",
        "\n",
        "            # Compare values\n",
        "            matches = sum(1 for g, e in zip(gt_values, ex_values)\n",
        "                         if str(g).lower().strip() == str(e).lower().strip())\n",
        "\n",
        "            if field not in comparison_results[\"field_accuracy\"]:\n",
        "                comparison_results[\"field_accuracy\"][field] = {\n",
        "                    \"matches\": 0,\n",
        "                    \"total\": 0,\n",
        "                    \"accuracy\": 0.0\n",
        "                }\n",
        "\n",
        "            comparison_results[\"field_accuracy\"][field][\"matches\"] += matches\n",
        "            comparison_results[\"field_accuracy\"][field][\"total\"] += max_len\n",
        "            comparison_results[\"total_fields_compared\"] += max_len\n",
        "            comparison_results[\"total_matches\"] += matches\n",
        "\n",
        "    # Calculate field-level accuracies\n",
        "    for field in comparison_results[\"field_accuracy\"]:\n",
        "        field_stats = comparison_results[\"field_accuracy\"][field]\n",
        "        field_stats[\"accuracy\"] = (\n",
        "            field_stats[\"matches\"] / field_stats[\"total\"]\n",
        "            if field_stats[\"total\"] > 0 else 0.0\n",
        "        )\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    comparison_results[\"overall_accuracy\"] = (\n",
        "        comparison_results[\"total_matches\"] / comparison_results[\"total_fields_compared\"]\n",
        "        if comparison_results[\"total_fields_compared\"] > 0 else 0.0\n",
        "    )\n",
        "\n",
        "    # Create matched DataFrame only for records with ground truth\n",
        "    matched_records = []\n",
        "    for _, ex_record in extracted_df.iterrows():\n",
        "        filename = ex_record['FileName']\n",
        "        if filename in ground_truth['FileName'].values:\n",
        "            matched_record = ex_record.to_dict()\n",
        "            matched_record['has_match'] = True\n",
        "            matched_record['record_accuracy'] = comparison_results[\"overall_accuracy\"]\n",
        "            matched_records.append(matched_record)\n",
        "\n",
        "    matched_df = pd.DataFrame(matched_records) if matched_records else pd.DataFrame()\n",
        "\n",
        "    return comparison_results, matched_df\n",
        "\n",
        "def process_single_paper_with_feedback(\n",
        "    pdf_path: str,\n",
        "    prompt_path: str,\n",
        "    ground_truth_data: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Process paper with feedback system\"\"\"\n",
        "    try:\n",
        "        # Initial setup\n",
        "        filename = normalize_filename(Path(pdf_path).stem)\n",
        "\n",
        "        # Validate ground truth data\n",
        "        if not ground_truth_data or 'patients' not in ground_truth_data:\n",
        "            logging.warning(f\"No valid ground truth data for {filename}\")\n",
        "            return None\n",
        "\n",
        "        pdf_text = extract_text_from_pdf(pdf_path)\n",
        "        base_prompt = read_prompt(prompt_path)\n",
        "\n",
        "        # Convert ground truth to DataFrame for comparison\n",
        "        ground_truth_records = []\n",
        "        for patient in ground_truth_data['patients']:\n",
        "            record = {\n",
        "                'PaperTitle': ground_truth_data['PaperTitle'],\n",
        "                'patientsNo': ground_truth_data['patientsNo'],\n",
        "                'FileName': filename,\n",
        "                **patient\n",
        "            }\n",
        "            ground_truth_records.append(record)\n",
        "        ground_truth_df = pd.DataFrame(ground_truth_records)\n",
        "\n",
        "        # Process with multiple attempts\n",
        "        max_attempts = 3\n",
        "        min_accuracy = 0.8\n",
        "        all_iterations_results = []\n",
        "        best_result = None\n",
        "        best_accuracy = 0\n",
        "\n",
        "        for attempt in range(max_attempts):\n",
        "            logging.info(f\"\\nAttempt {attempt + 1}/{max_attempts} for {filename}\")\n",
        "\n",
        "            try:\n",
        "                current_prompt = create_dynamic_prompt(\n",
        "                    base_prompt,\n",
        "                    all_iterations_results if all_iterations_results else None\n",
        "                )\n",
        "\n",
        "                model_output = ask_question(pdf_text, current_prompt)\n",
        "                metadata = clean_json_output(model_output)\n",
        "\n",
        "                if not metadata:\n",
        "                    logging.warning(f\"No valid metadata extracted in attempt {attempt + 1}\")\n",
        "                    continue\n",
        "\n",
        "                metadata['FileName'] = filename\n",
        "                standardized_records = convert_json_to_standardized_records(metadata)\n",
        "\n",
        "                comparison_results, matched_df = compare_with_ground_truth(\n",
        "                    standardized_records,\n",
        "                    ground_truth_df\n",
        "                )\n",
        "\n",
        "                if comparison_results is None:\n",
        "                    logging.warning(f\"Comparison failed in attempt {attempt + 1}\")\n",
        "                    continue\n",
        "\n",
        "                current_result = {\n",
        "                    'metadata': metadata,\n",
        "                    'accuracy': comparison_results['overall_accuracy'],\n",
        "                    'detailed_results': comparison_results,\n",
        "                    'prompt_used': current_prompt,\n",
        "                    'attempt': attempt + 1,\n",
        "                    'standardized_records': standardized_records\n",
        "                }\n",
        "\n",
        "                all_iterations_results.append(current_result)\n",
        "                logging.info(f\"Attempt {attempt + 1} accuracy: {comparison_results['overall_accuracy']:.2f}\")\n",
        "\n",
        "                if comparison_results['overall_accuracy'] > best_accuracy:\n",
        "                    best_accuracy = comparison_results['overall_accuracy']\n",
        "                    best_result = {\n",
        "                        'metadata': metadata,\n",
        "                        'iterations_results': all_iterations_results,\n",
        "                        'mean_accuracy': comparison_results['overall_accuracy'],\n",
        "                        'prompt_evolution': current_prompt,\n",
        "                        'final_detailed_results': comparison_results,\n",
        "                        'standardized_output': matched_df\n",
        "                    }\n",
        "\n",
        "                if comparison_results['overall_accuracy'] >= min_accuracy:\n",
        "                    logging.info(f\"Reached minimum accuracy threshold: {comparison_results['overall_accuracy']:.2f}\")\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in attempt {attempt + 1}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return best_result\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {pdf_path}: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "NjqiX-YD4PTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_json_output(response_text: str) -> Dict[str, Any]:\n",
        "    \"\"\"Clean and parse JSON output from Claude's response.\"\"\"\n",
        "    if not response_text:\n",
        "        logging.error(\"Empty response text received\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Remove any markdown code block indicators\n",
        "        cleaned_text = response_text.replace('```json', '').replace('```', '')\n",
        "\n",
        "        # Find the main JSON object\n",
        "        json_start = cleaned_text.find('{')\n",
        "        json_end = cleaned_text.rfind('}')\n",
        "\n",
        "        if json_start == -1 or json_end == -1:\n",
        "            logging.error(\"No JSON structure found in response\")\n",
        "            return None\n",
        "\n",
        "        # Extract and clean the JSON content\n",
        "        json_content = cleaned_text[json_start:json_end + 1]\n",
        "\n",
        "        # Parse the JSON\n",
        "        parsed_json = json.loads(json_content)\n",
        "\n",
        "        # Validate required fields\n",
        "        if not parsed_json.get('PaperTitle'):\n",
        "            logging.error(\"Missing PaperTitle in parsed JSON\")\n",
        "            return None\n",
        "\n",
        "        if not parsed_json.get('patientsINFO') and not parsed_json.get('patients'):\n",
        "            logging.error(\"Missing patient information in parsed JSON\")\n",
        "            return None\n",
        "\n",
        "        # Standardize patients field\n",
        "        if 'patients' in parsed_json:\n",
        "            parsed_json['patientsINFO'] = parsed_json.pop('patients')\n",
        "\n",
        "        return parsed_json\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        logging.error(f\"JSON parsing error: {str(e)}\")\n",
        "        logging.debug(f\"Attempted to parse content: {cleaned_text}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in clean_json_output: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def compare_with_ground_truth(\n",
        "    extracted_records: List[Dict[str, Any]],\n",
        "    ground_truth_df: pd.DataFrame\n",
        ") -> Tuple[Dict[str, Any], pd.DataFrame]:\n",
        "    \"\"\"Compare extracted records with ground truth.\"\"\"\n",
        "    comparison_results = {\n",
        "        \"matched_records\": 0,\n",
        "        \"total_records\": len(ground_truth_df),\n",
        "        \"field_accuracy\": {},\n",
        "        \"total_matches\": 0,\n",
        "        \"total_fields\": 0,\n",
        "        \"overall_accuracy\": 0.0\n",
        "    }\n",
        "\n",
        "    if not extracted_records:\n",
        "        logging.error(\"No extracted records to compare\")\n",
        "        return comparison_results, pd.DataFrame()\n",
        "\n",
        "    # Convert to DataFrame for comparison\n",
        "    extracted_df = pd.DataFrame(extracted_records)\n",
        "\n",
        "    # Define fields to compare\n",
        "    fields_to_compare = [\n",
        "        'PaperTitle', 'patientsNo', 'Diagnose', 'patientsNoAHP',\n",
        "        'AHPType', 'AHPDirection', 'AHPDegree', 'Eye',\n",
        "        'EyeMisalignment', 'DegreePD'\n",
        "    ]\n",
        "\n",
        "    # Compare each field\n",
        "    for field in fields_to_compare:\n",
        "        if field not in extracted_df.columns or field not in ground_truth_df.columns:\n",
        "            comparison_results[\"field_accuracy\"][field] = {\n",
        "                \"matches\": 0,\n",
        "                \"total\": len(ground_truth_df),\n",
        "                \"accuracy\": 0.0\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        gt_values = ground_truth_df[field].fillna('').astype(str)\n",
        "        ex_values = extracted_df[field].fillna('').astype(str)\n",
        "\n",
        "        # Pad shorter series\n",
        "        max_len = max(len(gt_values), len(ex_values))\n",
        "        gt_values = gt_values.tolist() + [''] * (max_len - len(gt_values))\n",
        "        ex_values = ex_values.tolist() + [''] * (max_len - len(ex_values))\n",
        "\n",
        "        # Count matches\n",
        "        matches = sum(1 for g, e in zip(gt_values, ex_values)\n",
        "                     if str(g).lower().strip() == str(e).lower().strip())\n",
        "\n",
        "        comparison_results[\"field_accuracy\"][field] = {\n",
        "            \"matches\": matches,\n",
        "            \"total\": max_len,\n",
        "            \"accuracy\": matches / max_len if max_len > 0 else 0.0\n",
        "        }\n",
        "\n",
        "        comparison_results[\"total_matches\"] += matches\n",
        "        comparison_results[\"total_fields\"] += max_len\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    comparison_results[\"overall_accuracy\"] = (\n",
        "        comparison_results[\"total_matches\"] / comparison_results[\"total_fields\"]\n",
        "        if comparison_results[\"total_fields\"] > 0 else 0.0\n",
        "    )\n",
        "\n",
        "    # Create matched records DataFrame\n",
        "    matched_records = []\n",
        "    for idx, record in enumerate(extracted_records):\n",
        "        if idx < len(ground_truth_df):\n",
        "            record['has_match'] = True\n",
        "            record['match_accuracy'] = comparison_results[\"overall_accuracy\"]\n",
        "            matched_records.append(record)\n",
        "\n",
        "    return comparison_results, pd.DataFrame(matched_records)\n",
        "\n",
        "def print_comparison_results(results: Dict[str, Any]) -> None:\n",
        "    \"\"\"Print formatted comparison results.\"\"\"\n",
        "    if not results:\n",
        "        print(\"No comparison results available\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nComparison Results:\")\n",
        "    print(f\"Records compared: {results.get('total_records', 0)}\")\n",
        "    print(f\"Overall Accuracy: {results.get('overall_accuracy', 0):.2%}\")\n",
        "\n",
        "    print(\"\\nField-level Accuracy:\")\n",
        "    for field, metrics in results.get('field_accuracy', {}).items():\n",
        "        accuracy = metrics.get('accuracy', 0)\n",
        "        matches = metrics.get('matches', 0)\n",
        "        total = metrics.get('total', 0)\n",
        "        print(f\"{field}: {accuracy:.2%} ({matches}/{total})\")\n",
        "\n",
        "def process_and_save_results(\n",
        "    paper_result: Dict[str, Any],\n",
        "    filename: str,\n",
        "    output_dir: str\n",
        ") -> None:\n",
        "    \"\"\"Process and save paper results with error handling.\"\"\"\n",
        "    try:\n",
        "        if not paper_result or 'final_detailed_results' not in paper_result:\n",
        "            logging.error(f\"Invalid paper result for {filename}\")\n",
        "            return\n",
        "\n",
        "        output_path = os.path.join(output_dir, f\"{filename}_result.json\")\n",
        "\n",
        "        result_data = {\n",
        "            'metadata': paper_result.get('metadata', {}),\n",
        "            'detailed_results': paper_result['final_detailed_results'],\n",
        "            'accuracy': paper_result.get('mean_accuracy', 0.0),\n",
        "            'iterations': [\n",
        "                {\n",
        "                    'attempt': r.get('attempt', 0),\n",
        "                    'accuracy': r.get('accuracy', 0.0),\n",
        "                    'prompt': r.get('prompt_used', '')\n",
        "                }\n",
        "                for r in paper_result.get('iterations_results', [])\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(result_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        logging.info(f\"Results saved to {output_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving results for {filename}: {str(e)}\")\n",
        "\n",
        "def convert_json_to_standardized_records(metadata: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Convert JSON output to list of standardized records with error handling.\"\"\"\n",
        "    try:\n",
        "        if not metadata:\n",
        "            return []\n",
        "\n",
        "        filename = metadata.get('FileName', '')\n",
        "        patients_info = metadata.get('patientsINFO', [])\n",
        "\n",
        "        # Handle case where patientsINFO is not a list\n",
        "        if not isinstance(patients_info, list):\n",
        "            patients_info = [patients_info]\n",
        "\n",
        "        standardized_records = []\n",
        "        for patient in patients_info:\n",
        "            if not isinstance(patient, dict):\n",
        "                continue\n",
        "\n",
        "            record = {\n",
        "                'FileName': filename,\n",
        "                'PaperTitle': metadata.get('PaperTitle', ''),\n",
        "                'patientsNo': metadata.get('patientsNo', ''),\n",
        "                'Diagnose': patient.get('Diagnose', ''),\n",
        "                'patientsNoAHP': patient.get('patientsNoAHP', ''),\n",
        "                'AHPType': patient.get('AHPType', ''),\n",
        "                'AHPDirection': patient.get('AHPDirection', ''),\n",
        "                'AHPDegree': patient.get('AHPDegree', ''),\n",
        "                'Eye': patient.get('eye', patient.get('Eye', '')),\n",
        "                'EyeMisalignment': patient.get('eyeMisalignment', patient.get('EyeMisalignment', '')),\n",
        "                'DegreePD': patient.get('degree', patient.get('DegreePD', ''))\n",
        "            }\n",
        "            standardized_records.append(record)\n",
        "\n",
        "        return standardized_records\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error converting JSON to standardized records: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def process_single_paper_with_feedback(\n",
        "    pdf_path: str,\n",
        "    prompt_path: str,\n",
        "    ground_truth_data: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Process paper with feedback system and improved error handling.\"\"\"\n",
        "    try:\n",
        "        filename = normalize_filename(Path(pdf_path).stem)\n",
        "\n",
        "        if not ground_truth_data or 'patients' not in ground_truth_data:\n",
        "            logging.warning(f\"No valid ground truth data for {filename}\")\n",
        "            return None\n",
        "\n",
        "        pdf_text = extract_text_from_pdf(pdf_path)\n",
        "        base_prompt = read_prompt(prompt_path)\n",
        "\n",
        "        ground_truth_records = []\n",
        "        for patient in ground_truth_data['patients']:\n",
        "            record = {\n",
        "                'PaperTitle': ground_truth_data.get('PaperTitle', ''),\n",
        "                'patientsNo': ground_truth_data.get('patientsNo', ''),\n",
        "                'FileName': filename,\n",
        "                **{k: v if v is not None else '' for k, v in patient.items()}\n",
        "            }\n",
        "            ground_truth_records.append(record)\n",
        "\n",
        "        if not ground_truth_records:\n",
        "            logging.warning(f\"No ground truth records created for {filename}\")\n",
        "            return None\n",
        "\n",
        "        ground_truth_df = pd.DataFrame(ground_truth_records)\n",
        "\n",
        "        max_attempts = 3\n",
        "        min_accuracy = 0.8\n",
        "        all_iterations_results = []\n",
        "        best_result = None\n",
        "        best_accuracy = 0\n",
        "\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                current_prompt = create_dynamic_prompt(\n",
        "                    base_prompt,\n",
        "                    all_iterations_results if all_iterations_results else None\n",
        "                )\n",
        "\n",
        "                model_output = ask_question(pdf_text, current_prompt)\n",
        "                if not model_output:\n",
        "                    continue\n",
        "\n",
        "                metadata = clean_json_output(model_output)\n",
        "                if not metadata:\n",
        "                    continue\n",
        "\n",
        "                metadata['FileName'] = filename\n",
        "                standardized_records = convert_json_to_standardized_records(metadata)\n",
        "\n",
        "                if not standardized_records:\n",
        "                    continue\n",
        "\n",
        "                comparison_results, matched_df = compare_with_ground_truth(\n",
        "                    standardized_records,\n",
        "                    ground_truth_df\n",
        "                )\n",
        "\n",
        "                if not comparison_results:\n",
        "                    continue\n",
        "\n",
        "                accuracy = comparison_results.get('overall_accuracy', 0.0)\n",
        "\n",
        "                current_result = {\n",
        "                    'metadata': metadata,\n",
        "                    'accuracy': accuracy,\n",
        "                    'detailed_results': comparison_results,\n",
        "                    'prompt_used': current_prompt,\n",
        "                    'attempt': attempt + 1,\n",
        "                    'standardized_records': standardized_records\n",
        "                }\n",
        "\n",
        "                all_iterations_results.append(current_result)\n",
        "                logging.info(f\"Attempt {attempt + 1} accuracy: {accuracy:.2f}\")\n",
        "\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_result = {\n",
        "                        'metadata': metadata,\n",
        "                        'iterations_results': all_iterations_results,\n",
        "                        'mean_accuracy': accuracy,\n",
        "                        'prompt_evolution': current_prompt,\n",
        "                        'final_detailed_results': comparison_results,\n",
        "                        'standardized_output': matched_df\n",
        "                    }\n",
        "\n",
        "                if accuracy >= min_accuracy:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error in attempt {attempt + 1}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return best_result\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing {pdf_path}: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "dYBtPe5H5C_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results_to_csv(json_results: Dict[str, Any], output_dir: str, filename: str) -> None:\n",
        "    \"\"\"Save JSON results to CSV with standardized values.\"\"\"\n",
        "    try:\n",
        "        all_records = []\n",
        "        metadata = json_results.get('metadata', {})\n",
        "        patients_info = metadata.get('patientsINFO', [])\n",
        "\n",
        "        if not isinstance(patients_info, list):\n",
        "            patients_info = [patients_info]\n",
        "\n",
        "        # Process each patient with standardization\n",
        "        for patient in patients_info:\n",
        "            record = {\n",
        "                'FileName': normalize_filename(filename),\n",
        "                'PaperTitle': metadata.get('PaperTitle', ''),\n",
        "                'patientsNo': standardize_patient_count(metadata.get('patientsNo', '')),\n",
        "                'Diagnose': standardize_disease_names(patient.get('Diagnose', '')),\n",
        "                'patientsNoAHP': standardize_patient_count(patient.get('patientsNoAHP', '')),\n",
        "                'AHPType': standardize_ahp_type(patient.get('AHPType', '')),\n",
        "                'AHPDirection': patient.get('AHPDirection', '').lower().strip(),\n",
        "                'AHPDegree': standardize_degree_values(patient.get('AHPDegree', '')),\n",
        "                'Eye': standardize_eye(patient.get('eye', patient.get('Eye', ''))),\n",
        "                'EyeMisalignment': patient.get('eyeMisalignment', '').lower().strip(),\n",
        "                'DegreePD': standardize_degree_values(patient.get('degree', patient.get('DegreePD', ''))),\n",
        "                'Extraction_Accuracy': json_results.get('accuracy', 0.0)\n",
        "            }\n",
        "            all_records.append(record)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(all_records)\n",
        "\n",
        "        # Create CSV filename for standardized data\n",
        "        csv_path = os.path.join(output_dir, 'standardized_extracted_data.csv')\n",
        "\n",
        "        # If file exists, append without header; if not, create with header\n",
        "        if os.path.exists(csv_path):\n",
        "            df.to_csv(csv_path, mode='a', header=False, index=False)\n",
        "        else:\n",
        "            df.to_csv(csv_path, index=False)\n",
        "\n",
        "        logging.info(f\"Standardized results saved to: {csv_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving standardized results to CSV for {filename}: {str(e)}\")\n",
        "\n",
        "def process_all_json_to_csv(json_dir: str, output_dir: str) -> None:\n",
        "    \"\"\"Process all JSON files in directory and combine into single standardized CSV.\"\"\"\n",
        "    try:\n",
        "        all_records = []\n",
        "\n",
        "        for filename in os.listdir(json_dir):\n",
        "            if not filename.endswith('_result.json'):\n",
        "                continue\n",
        "\n",
        "            file_path = os.path.join(json_dir, filename)\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    json_data = json.load(f)\n",
        "\n",
        "                base_filename = normalize_filename(filename.replace('_result.json', ''))\n",
        "                metadata = json_data.get('metadata', {})\n",
        "                patients_info = metadata.get('patientsINFO', [])\n",
        "\n",
        "                if not isinstance(patients_info, list):\n",
        "                    patients_info = [patients_info]\n",
        "\n",
        "                # Process each patient with standardization\n",
        "                for patient in patients_info:\n",
        "                    record = {\n",
        "                        'FileName': base_filename,\n",
        "                        'PaperTitle': metadata.get('PaperTitle', ''),\n",
        "                        'patientsNo': standardize_patient_count(metadata.get('patientsNo', '')),\n",
        "                        'Diagnose': standardize_disease_names(patient.get('Diagnose', '')),\n",
        "                        'patientsNoAHP': standardize_patient_count(patient.get('patientsNoAHP', '')),\n",
        "                        'AHPType': standardize_ahp_type(patient.get('AHPType', '')),\n",
        "                        'AHPDirection': patient.get('AHPDirection', '').lower().strip(),\n",
        "                        'AHPDegree': standardize_degree_values(patient.get('AHPDegree', '')),\n",
        "                        'Eye': standardize_eye(patient.get('eye', patient.get('Eye', ''))),\n",
        "                        'EyeMisalignment': patient.get('eyeMisalignment', '').lower().strip(),\n",
        "                        'DegreePD': standardize_degree_values(patient.get('degree', patient.get('DegreePD', ''))),\n",
        "                        'Extraction_Accuracy': json_data.get('accuracy', 0.0)\n",
        "                    }\n",
        "                    all_records.append(record)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing {filename}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if all_records:\n",
        "            # Convert to DataFrame and save standardized CSV\n",
        "            df = pd.DataFrame(all_records)\n",
        "            csv_path = os.path.join(output_dir, 'standardized_extracted_data.csv')\n",
        "            df.to_csv(csv_path, index=False)\n",
        "\n",
        "            logging.info(f\"Standardized combined results saved to: {csv_path}\")\n",
        "            logging.info(f\"Total records processed: {len(all_records)}\")\n",
        "        else:\n",
        "            logging.warning(\"No records found to process\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing JSON files to CSV: {str(e)}\")"
      ],
      "metadata": {
        "id": "UbY3WEccT4SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_csv_with_ground_truth(\n",
        "    extracted_csv_path: str,\n",
        "    ground_truth_csv_path: str,\n",
        "    output_dir: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Compare extracted CSV with ground truth CSV and generate comparison report.\n",
        "\n",
        "    Args:\n",
        "        extracted_csv_path: Path to the extracted data CSV\n",
        "        ground_truth_csv_path: Path to the ground truth CSV\n",
        "        output_dir: Directory to save comparison results\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing comparison metrics\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load CSVs\n",
        "        extracted_df = pd.read_csv(extracted_csv_path)\n",
        "        ground_truth_df = pd.read_csv(ground_truth_csv_path)\n",
        "\n",
        "        # Normalize filenames\n",
        "        extracted_df['FileName'] = extracted_df['FileName'].astype(str).apply(normalize_filename)\n",
        "        ground_truth_df['FileName'] = ground_truth_df['FileName'].astype(str).apply(normalize_filename)\n",
        "\n",
        "        # Initialize results\n",
        "        comparison_results = {\n",
        "            'overall_accuracy': 0.0,\n",
        "            'field_accuracy': {},\n",
        "            'file_level_results': {},\n",
        "            'mismatches': [],\n",
        "            'summary': {\n",
        "                'total_files': len(extracted_df['FileName'].unique()),\n",
        "                'matched_files': 0,\n",
        "                'total_fields_compared': 0,\n",
        "                'total_matches': 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Fields to compare\n",
        "        fields_to_compare = [\n",
        "            'PaperTitle', 'patientsNo', 'Diagnose', 'patientsNoAHP',\n",
        "            'AHPType', 'AHPDirection', 'AHPDegree', 'Eye',\n",
        "            'EyeMisalignment', 'DegreePD'\n",
        "        ]\n",
        "\n",
        "        # Compare file by file\n",
        "        all_detailed_comparisons = []\n",
        "\n",
        "        for filename in extracted_df['FileName'].unique():\n",
        "            extracted_file = extracted_df[extracted_df['FileName'] == filename]\n",
        "            ground_truth_file = ground_truth_df[ground_truth_df['FileName'] == filename]\n",
        "\n",
        "            if ground_truth_file.empty:\n",
        "                logging.warning(f\"No ground truth found for file {filename}\")\n",
        "                continue\n",
        "\n",
        "            comparison_results['summary']['matched_files'] += 1\n",
        "            file_results = {'filename': filename, 'field_matches': {}, 'mismatches': []}\n",
        "\n",
        "            # Compare each field\n",
        "            for field in fields_to_compare:\n",
        "                ext_values = extracted_file[field].fillna('').astype(str).tolist()\n",
        "                gt_values = ground_truth_file[field].fillna('').astype(str).tolist()\n",
        "\n",
        "                # Pad shorter list with empty strings\n",
        "                max_len = max(len(ext_values), len(gt_values))\n",
        "                ext_values.extend([''] * (max_len - len(ext_values)))\n",
        "                gt_values.extend([''] * (max_len - len(gt_values)))\n",
        "\n",
        "                # Compare values\n",
        "                field_matches = 0\n",
        "                field_mismatches = []\n",
        "\n",
        "                for idx, (ext_val, gt_val) in enumerate(zip(ext_values, gt_values)):\n",
        "                    ext_val = str(ext_val).lower().strip()\n",
        "                    gt_val = str(gt_val).lower().strip()\n",
        "\n",
        "                    if ext_val == gt_val:\n",
        "                        field_matches += 1\n",
        "                    else:\n",
        "                        field_mismatches.append({\n",
        "                            'patient_index': idx + 1,\n",
        "                            'extracted': ext_val,\n",
        "                            'ground_truth': gt_val\n",
        "                        })\n",
        "\n",
        "                field_accuracy = field_matches / max_len if max_len > 0 else 0\n",
        "\n",
        "                # Update field results\n",
        "                file_results['field_matches'][field] = {\n",
        "                    'matches': field_matches,\n",
        "                    'total': max_len,\n",
        "                    'accuracy': field_accuracy\n",
        "                }\n",
        "\n",
        "                if field_mismatches:\n",
        "                    file_results['mismatches'].append({\n",
        "                        'field': field,\n",
        "                        'mismatches': field_mismatches\n",
        "                    })\n",
        "\n",
        "                # Update global statistics\n",
        "                if field not in comparison_results['field_accuracy']:\n",
        "                    comparison_results['field_accuracy'][field] = {\n",
        "                        'matches': 0,\n",
        "                        'total': 0\n",
        "                    }\n",
        "                comparison_results['field_accuracy'][field]['matches'] += field_matches\n",
        "                comparison_results['field_accuracy'][field]['total'] += max_len\n",
        "\n",
        "                comparison_results['summary']['total_fields_compared'] += max_len\n",
        "                comparison_results['summary']['total_matches'] += field_matches\n",
        "\n",
        "            comparison_results['file_level_results'][filename] = file_results\n",
        "\n",
        "            # Create detailed comparison DataFrame for this file\n",
        "            detailed_comparison = []\n",
        "            for idx in range(max(len(extracted_file), len(ground_truth_file))):\n",
        "                row_comparison = {'FileName': filename, 'PatientIndex': idx + 1}\n",
        "                for field in fields_to_compare:\n",
        "                    ext_val = ext_values[idx] if idx < len(ext_values) else ''\n",
        "                    gt_val = gt_values[idx] if idx < len(gt_values) else ''\n",
        "                    row_comparison.update({\n",
        "                        f'{field}_Extracted': ext_val,\n",
        "                        f'{field}_GroundTruth': gt_val,\n",
        "                        f'{field}_Matches': ext_val.lower().strip() == gt_val.lower().strip()\n",
        "                    })\n",
        "                detailed_comparison.append(row_comparison)\n",
        "\n",
        "            all_detailed_comparisons.extend(detailed_comparison)\n",
        "\n",
        "        # Calculate overall accuracy\n",
        "        comparison_results['overall_accuracy'] = (\n",
        "            comparison_results['summary']['total_matches'] /\n",
        "            comparison_results['summary']['total_fields_compared']\n",
        "            if comparison_results['summary']['total_fields_compared'] > 0 else 0\n",
        "        )\n",
        "\n",
        "        # Calculate field-level accuracies\n",
        "        for field, stats in comparison_results['field_accuracy'].items():\n",
        "            stats['accuracy'] = (\n",
        "                stats['matches'] / stats['total'] if stats['total'] > 0 else 0\n",
        "            )\n",
        "\n",
        "        # Save detailed comparison to CSV\n",
        "        detailed_df = pd.DataFrame(all_detailed_comparisons)\n",
        "        detailed_csv_path = os.path.join(output_dir, 'detailed_comparison.csv')\n",
        "        detailed_df.to_csv(detailed_csv_path, index=False)\n",
        "\n",
        "        # Save summary report to JSON\n",
        "        summary_path = os.path.join(output_dir, 'comparison_summary.json')\n",
        "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(comparison_results, f, indent=4)\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\nComparison Summary:\")\n",
        "        print(f\"Total Files: {comparison_results['summary']['total_files']}\")\n",
        "        print(f\"Matched Files: {comparison_results['summary']['matched_files']}\")\n",
        "        print(f\"Overall Accuracy: {comparison_results['overall_accuracy']:.2%}\")\n",
        "        print(\"\\nField-level Accuracy:\")\n",
        "        for field, stats in comparison_results['field_accuracy'].items():\n",
        "            print(f\"{field}: {stats['accuracy']:.2%} ({stats['matches']}/{stats['total']})\")\n",
        "\n",
        "        print(f\"\\nDetailed comparison saved to: {detailed_csv_path}\")\n",
        "        print(f\"Summary report saved to: {summary_path}\")\n",
        "\n",
        "        return comparison_results\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in CSV comparison: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "7U68LqxnMrJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_detailed_accuracy(results: Dict[str, Any]) -> None:\n",
        "    \"\"\"Print detailed accuracy metrics for extraction results.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"DETAILED ACCURACY REPORT\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if not results:\n",
        "        print(\"No results available\")\n",
        "        return\n",
        "\n",
        "    # Overall statistics\n",
        "    print(\"\\nOVERALL STATISTICS:\")\n",
        "    print(f\"Total papers processed: {results.get('total_processed', 0)}\")\n",
        "    print(f\"Successful extractions: {results.get('successful_extractions', 0)}\")\n",
        "    print(f\"Failed extractions: {results.get('failed_extractions', 0)}\")\n",
        "    print(f\"Overall accuracy: {results.get('overall_accuracy', 0):.2%}\")\n",
        "\n",
        "    # Process individual paper results\n",
        "    print(\"\\nPAPER-LEVEL RESULTS:\")\n",
        "    for paper in results.get('paper_results', []):\n",
        "        print(f\"\\nFile: {paper.get('filename', 'Unknown')}\")\n",
        "        if paper.get('success', False):\n",
        "            print(f\"Status: Success\")\n",
        "            print(f\"Accuracy: {paper.get('accuracy', 0):.2%}\")\n",
        "\n",
        "            # Print field-level accuracy if available\n",
        "            detailed_results = paper.get('detailed_results', {})\n",
        "            if detailed_results:\n",
        "                print(\"\\nField-level accuracy:\")\n",
        "                for field, metrics in detailed_results.get('field_accuracy', {}).items():\n",
        "                    matches = metrics.get('matches', 0)\n",
        "                    total = metrics.get('total', 0)\n",
        "                    accuracy = metrics.get('accuracy', 0)\n",
        "                    print(f\"  {field}: {accuracy:.2%} ({matches}/{total})\")\n",
        "\n",
        "                # Print mismatches if any\n",
        "                mismatches = detailed_results.get('mismatches', [])\n",
        "                if mismatches:\n",
        "                    print(\"\\nMismatches:\")\n",
        "                    for mismatch in mismatches:\n",
        "                        field = mismatch.get('field', '')\n",
        "                        details = mismatch.get('mismatches', [])\n",
        "                        print(f\"\\n  {field}:\")\n",
        "                        for detail in details:\n",
        "                            print(f\"    Expected: {detail.get('ground_truth', '')}\")\n",
        "                            print(f\"    Got: {detail.get('extracted', '')}\")\n",
        "        else:\n",
        "            print(f\"Status: Failed\")\n",
        "            print(f\"Error: {paper.get('error', 'Unknown error')}\")\n",
        "\n",
        "def save_results_to_csv(json_results: Dict[str, Any], output_dir: str, filename: str) -> None:\n",
        "    \"\"\"Save JSON results to CSV with updated standardization.\"\"\"\n",
        "    try:\n",
        "        all_records = []\n",
        "        metadata = json_results.get('metadata', {})\n",
        "        patients_info = metadata.get('patientsINFO', [])\n",
        "\n",
        "        if not isinstance(patients_info, list):\n",
        "            patients_info = [patients_info]\n",
        "\n",
        "        for patient in patients_info:\n",
        "            record = {\n",
        "                'FileName': normalize_filename(filename),\n",
        "                'PaperTitle': metadata.get('PaperTitle', ''),\n",
        "                'patientsNo': standardize_patient_count(metadata.get('patientsNo', '')),\n",
        "                'Diagnose': standardize_disease_names(patient.get('Diagnose', '')),\n",
        "                'patientsNoAHP': standardize_patient_count(patient.get('patientsNoAHP', '')),\n",
        "                'AHPType': standardize_ahp_type(patient.get('AHPType', '')),\n",
        "                'AHPDirection': patient.get('AHPDirection', '').lower().strip(),\n",
        "                'AHPDegree': standardize_degree_values(patient.get('AHPDegree', '')),\n",
        "                'Eye': standardize_eye(patient.get('eye', patient.get('Eye', ''))),\n",
        "                'EyeMisalignment': patient.get('eyeMisalignment', '').lower().strip(),\n",
        "                'DegreePD': standardize_degree_values(patient.get('degree', patient.get('DegreePD', ''))),\n",
        "                'Extraction_Accuracy': json_results.get('accuracy', 0.0)\n",
        "            }\n",
        "            all_records.append(record)\n",
        "\n",
        "        df = pd.DataFrame(all_records)\n",
        "        csv_path = os.path.join(output_dir, 'standardized_extracted_data.csv')\n",
        "\n",
        "        if os.path.exists(csv_path):\n",
        "            df.to_csv(csv_path, mode='a', header=False, index=False)\n",
        "        else:\n",
        "            df.to_csv(csv_path, index=False)\n",
        "\n",
        "        logging.info(f\"Standardized results saved to: {csv_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving standardized results to CSV for {filename}: {str(e)}\")\n",
        "\n",
        "\n",
        "def clean_json_output(response_text: str) -> Dict[str, Any]:\n",
        "    \"\"\"Clean and parse JSON output from Claude's response.\"\"\"\n",
        "    if not response_text:\n",
        "        logging.error(\"Empty response text received\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Remove markdown code blocks and leading/trailing whitespace\n",
        "        cleaned_text = response_text.replace('```json', '').replace('```', '').strip()\n",
        "\n",
        "        # Find the first complete JSON object\n",
        "        json_start = cleaned_text.find('{')\n",
        "        if json_start == -1:\n",
        "            logging.error(\"No JSON object found\")\n",
        "            return None\n",
        "\n",
        "        # Track brackets to find the matching closing brace\n",
        "        bracket_count = 0\n",
        "        in_string = False\n",
        "        escape_char = False\n",
        "\n",
        "        for i in range(json_start, len(cleaned_text)):\n",
        "            char = cleaned_text[i]\n",
        "\n",
        "            # Handle string literals\n",
        "            if char == '\"' and not escape_char:\n",
        "                in_string = not in_string\n",
        "            # Handle escape characters\n",
        "            elif char == '\\\\' and not escape_char:\n",
        "                escape_char = True\n",
        "                continue\n",
        "            # Reset escape flag\n",
        "            escape_char = False\n",
        "\n",
        "            # Only count brackets outside of strings\n",
        "            if not in_string:\n",
        "                if char == '{':\n",
        "                    bracket_count += 1\n",
        "                elif char == '}':\n",
        "                    bracket_count -= 1\n",
        "                    # Found matching closing brace\n",
        "                    if bracket_count == 0:\n",
        "                        json_str = cleaned_text[json_start:i+1]\n",
        "                        try:\n",
        "                            return json.loads(json_str)\n",
        "                        except json.JSONDecodeError as e:\n",
        "                            logging.error(f\"Failed to parse extracted JSON: {e}\")\n",
        "                            return None\n",
        "\n",
        "        logging.error(\"No complete JSON object found\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in clean_json_output: {str(e)}\")\n",
        "        logging.debug(f\"Original text: {response_text}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "mHHvl1pj9zHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function with CSV generation and comparison\"\"\"\n",
        "    # Setup paths\n",
        "    base_dir = \"/content/drive/MyDrive/Body-Language-Project/Data Extraction\"\n",
        "    paths = {\n",
        "        'prompt': f\"{base_dir}/Files/Hierarchical prompt.txt\",\n",
        "        'pdf_dir': f\"{base_dir}/Ocular PDFs\",\n",
        "        'output_dir': f\"{base_dir}/Extracted JSON/Claude Sonnet 3.5/Hierarchical Prompt\",\n",
        "        'ground_truth': f\"{base_dir}/Files/transformed_ground_data-Revised-with-filename.csv\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Initialize logging\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.StreamHandler(),\n",
        "                logging.FileHandler('extraction.log')\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Load ground truth data\n",
        "        ground_truth_df = pd.read_csv(paths['ground_truth'])\n",
        "        ground_truth_df = ground_truth_df.assign(\n",
        "            FileName=lambda x: x['FileName'].astype(str).apply(normalize_filename)\n",
        "        )\n",
        "        logging.info(f\"Loaded {len(ground_truth_df)} ground truth records\")\n",
        "\n",
        "        # Get PDF files\n",
        "        pdf_files = sorted([\n",
        "            f for f in os.listdir(paths['pdf_dir'])\n",
        "            if f.endswith('.pdf')\n",
        "        ])\n",
        "        logging.info(f\"Found {len(pdf_files)} PDF files\")\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(paths['output_dir'], exist_ok=True)\n",
        "\n",
        "        # Initialize results tracking\n",
        "        results = {\n",
        "            'total_processed': 0,\n",
        "            'successful_extractions': 0,\n",
        "            'failed_extractions': 0,\n",
        "            'overall_accuracy': 0.0,\n",
        "            'paper_results': []\n",
        "        }\n",
        "\n",
        "        # Process each PDF\n",
        "        for pdf_file in pdf_files[:10]:  # Process first 10 files\n",
        "            pdf_path = os.path.join(paths['pdf_dir'], pdf_file)\n",
        "            filename = normalize_filename(Path(pdf_file).stem)\n",
        "\n",
        "            logging.info(f\"\\nProcessing paper: {filename}\")\n",
        "\n",
        "            # Get ground truth for this file\n",
        "            ground_truth_rows = ground_truth_df[ground_truth_df['FileName'] == filename]\n",
        "            if ground_truth_rows.empty:\n",
        "                logging.info(f\"Skipping {filename} - no ground truth data\")\n",
        "                continue\n",
        "\n",
        "            ground_truth = {\n",
        "                'PaperTitle': ground_truth_rows.iloc[0]['PaperTitle'],\n",
        "                'patientsNo': ground_truth_rows.iloc[0]['patientsNo'],\n",
        "                'patients': []\n",
        "            }\n",
        "\n",
        "            for _, row in ground_truth_rows.iterrows():\n",
        "                patient = {k: v for k, v in row.items()\n",
        "                         if k not in ['FileName', 'PaperTitle', 'patientsNo']}\n",
        "                ground_truth['patients'].append(patient)\n",
        "\n",
        "            # Process the paper\n",
        "            paper_result = process_single_paper_with_feedback(\n",
        "                pdf_path=pdf_path,\n",
        "                prompt_path=paths['prompt'],\n",
        "                ground_truth_data=ground_truth\n",
        "            )\n",
        "\n",
        "            results['total_processed'] += 1\n",
        "\n",
        "            if paper_result and isinstance(paper_result, dict):\n",
        "                results['successful_extractions'] += 1\n",
        "                accuracy = paper_result.get('mean_accuracy', 0.0)\n",
        "                results['overall_accuracy'] += accuracy\n",
        "\n",
        "                # Save JSON result\n",
        "                output_path = os.path.join(paths['output_dir'], f\"{filename}_result.json\")\n",
        "                with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump({\n",
        "                        'metadata': paper_result['metadata'],\n",
        "                        'detailed_results': paper_result['final_detailed_results'],\n",
        "                        'accuracy': accuracy,\n",
        "                        'iterations': [\n",
        "                            {\n",
        "                                'attempt': r['attempt'],\n",
        "                                'accuracy': r['accuracy'],\n",
        "                                'prompt': r['prompt_used']\n",
        "                            }\n",
        "                            for r in paper_result['iterations_results']\n",
        "                        ]\n",
        "                    }, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "                # Save to CSV immediately\n",
        "                save_results_to_csv(\n",
        "                    json_results={\n",
        "                        'metadata': paper_result['metadata'],\n",
        "                        'accuracy': accuracy\n",
        "                    },\n",
        "                    output_dir=paths['output_dir'],\n",
        "                    filename=filename\n",
        "                )\n",
        "\n",
        "                results['paper_results'].append({\n",
        "                    'filename': filename,\n",
        "                    'accuracy': accuracy,\n",
        "                    'success': True,\n",
        "                    'detailed_results': paper_result['final_detailed_results']\n",
        "                })\n",
        "\n",
        "                logging.info(f\"Processed {filename} with accuracy: {accuracy:.2%}\")\n",
        "\n",
        "            else:\n",
        "                results['failed_extractions'] += 1\n",
        "                results['paper_results'].append({\n",
        "                    'filename': filename,\n",
        "                    'success': False,\n",
        "                    'error': 'Failed to process paper'\n",
        "                })\n",
        "                logging.warning(f\"Failed to process {filename}\")\n",
        "\n",
        "        # Calculate final statistics\n",
        "        if results['successful_extractions'] > 0:\n",
        "            results['overall_accuracy'] = results['overall_accuracy'] / results['successful_extractions']\n",
        "\n",
        "        # Save final report\n",
        "        report_path = os.path.join(paths['output_dir'], 'extraction_report.json')\n",
        "        with open(report_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        # Process all JSON files to create combined CSV\n",
        "        logging.info(\"Creating combined CSV from all results...\")\n",
        "        process_all_json_to_csv(\n",
        "            json_dir=paths['output_dir'],\n",
        "            output_dir=paths['output_dir']\n",
        "        )\n",
        "\n",
        "        # Compare extracted data with ground truth\n",
        "        logging.info(\"Comparing results with ground truth...\")\n",
        "        extracted_csv_path = os.path.join(paths['output_dir'], 'standardized_extracted_data.csv')\n",
        "        comparison_results = compare_csv_with_ground_truth(\n",
        "            extracted_csv_path=extracted_csv_path,\n",
        "            ground_truth_csv_path=paths['ground_truth'],\n",
        "            output_dir=paths['output_dir']\n",
        "        )\n",
        "\n",
        "        # Print final summary\n",
        "        print(\"\\nProcessing Complete!\")\n",
        "        print(f\"Total papers processed: {results['total_processed']}\")\n",
        "        print(f\"Successful extractions: {results['successful_extractions']}\")\n",
        "        print(f\"Failed extractions: {results['failed_extractions']}\")\n",
        "        print(f\"Overall extraction accuracy: {results['overall_accuracy']:.2%}\")\n",
        "        print(\"\\nOutput files:\")\n",
        "        print(f\"1. JSON results: {paths['output_dir']}\")\n",
        "        print(f\"2. Combined CSV: {extracted_csv_path}\")\n",
        "        print(f\"3. Comparison results: {os.path.join(paths['output_dir'], 'detailed_comparison.csv')}\")\n",
        "        print(f\"4. Summary report: {os.path.join(paths['output_dir'], 'comparison_summary.json')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in main execution\", exc_info=True)\n",
        "        print(f\"Error in main execution: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bFFEak7eMrHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343776af-12fd-4a2c-969c-f372359b88ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw response from Claude: {\n",
            "    \"PaperTitle\": \"Poor Results After Recession of Both Medial Rectus Muscles in Unilateral Small-Angle Duane's Syndrome, Type I\",\n",
            "    \"patientsNo\": \"4\"\n",
            "}\n",
            "\n",
            "{\n",
            "    \"Diagnose\": \"Duane's syndrome, type I\",\n",
            "    \"patientsNoAHP\": \"4 patients\"\n",
            "}\n",
            "\n",
            "{\n",
            "    \"patientsInfo\": [\n",
            "        {\n",
            "            \"Diagnose\": \"Duane's syndrome, type I\",\n",
            "            \"patientsNoAHP\": \"1 patient\",\n",
            "            \"AHPType\": \"head turn\",\n",
            "            \"AHPDirection\": \"left\",\n",
            "            \"AHPDegree\": \"25 degrees\",\n",
            "            \"eye\": \"left\",\n",
            "            \"eyeMisalignment\": \"esotropia\",\n",
            "            \"degree\": \"12 PD\"\n",
            "        },\n",
            "        {\n",
            "            \"Diagnose\": \"Duane's syndrome, type I\", \n",
            "            \"patientsNoAHP\": \"1 patient\",\n",
            "            \"AHPType\": \"head turn\",\n",
            "            \"AHPDirection\": \"left\",\n",
            "            \"AHPDegree\": \"25 degrees\",\n",
            "            \"eye\": \"left\",\n",
            "            \"eyeMisalignment\": \"esotropia\",\n",
            "            \"degree\": \"4 PD\"\n",
            "        },\n",
            "        {\n",
            "            \"Diagnose\": \"Duane's syndrome, type I\",\n",
            "            \"patientsNoAHP\": \"1 patient\", \n",
            "            \"AHPType\": \"head turn\",\n",
            "            \"AHPDirection\": \"left\",\n",
            "            \"AHPDegree\": \"30 degrees\",\n",
            "            \"eye\": \"left\",\n",
            "            \"eyeMisalignment\": \"esotropia\",\n",
            "            \"degree\": \"16 PD\"\n",
            "        },\n",
            "        {\n",
            "            \"Diagnose\": \"Duane's syndrome, type I\",\n",
            "            \"patientsNoAHP\": \"1 patient\",\n",
            "            \"AHPType\": \"head turn\", \n",
            "            \"AHPDirection\": \"left\",\n",
            "            \"AHPDegree\": \"15 degrees\",\n",
            "            \"eye\": \"left\",\n",
            "            \"eyeMisalignment\": \"esotropia\",\n",
            "            \"degree\": \"10 PD\"\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error in attempt 3: 'int' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw response from Claude: Here is the extracted information in JSON format:\n",
            "\n",
            "{\n",
            "    \"PaperTitle\": \"Poor Results After Recession of Both Medial Rectus Muscles in Unilateral Small-Angle Duane's Syndrome, Type I\",\n",
            "    \"patientsNo\": \"4 patients\",\n",
            "    \"patientsINFO\": [\n",
            "        {\n",
            "            \"Diagnose\": \"Duane Retraction Syndrome (DRS), Type I\",\n",
            "            \"patientsNoAHP\": \"4 patients\",\n",
            "            \"AHPType\": \"head turn\",\n",
            "            \"AHPDirection\": \"left\",\n",
            "            \"AHPDegree\": \"25 degrees, 15 degrees, 30 degrees, 15 degrees\",\n",
            "            \"eye\": \"left\",\n",
            "            \"eyeMisalignment\": \"esotropia\",\n",
            "            \"degree\": \"12 PD, 4 PD, 16 PD, 10 PD\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "\n",
            "Key Points:\n",
            "\n",
            "- The paper is a case series examining 4 patients with unilateral small-angle Duane's syndrome Type I who underwent recession of both the affected eye's medial rectus muscle and the normal contralateral medial rectus muscle.\n",
            "\n",
            "- All 4 patients had preoperative left head turn/face turn to maintain binocularity due to the left eye being the affected Duane's eye with limited abduction. \n",
            "\n",
            "- Postoperatively, 3 out of 4 patients had poor results with persistence or worsening of the abnormal head posture. One patient developed consecutive exotropia.\n",
            "\n",
            "- The authors caution against routinely performing recession of the normal medial rectus in addition to the affected medial rectus in small-angle Duane's syndrome Type I, as it may worsen head posture or cause consecutive exotropia.\n",
            "\n",
            "- Specific measurements of head turn degree and angle of strabismus are provided for each patient pre- and post-operatively.\n",
            "\n",
            "Let me know if you need any clarification or have additional requests!\n",
            "Raw response from Claude: {\n",
            "    \"PaperTitle\": \"A Preliminary Study on the Outcome of Plication Augmentation of the Augmented Anderson Procedure for Patients with Infantile Nystagmus Syndrome and a Face Turn\",\n",
            "    \"patientsNo\": \"8\"\n",
            "}\n",
            "Raw response from Claude: {\n",
            "    \"PaperTitle\": \"A Preliminary Study on the Outcome of Plication Augmentation of the Augmented Anderson Procedure for Patients with Infantile Nystagmus Syndrome and a Face Turn\",\n",
            "    \"patientsNo\": \"8\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Failed to process 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw response from Claude: {\n",
            "    \"PaperTitle\": \"A Preliminary Study on the Outcome of Plication Augmentation of the Augmented Anderson Procedure for Patients with Infantile Nystagmus Syndrome and a Face Turn\",\n",
            "    \"patientsNo\": \"8\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error in attempt 2: 'int' object is not subscriptable\n",
            "ERROR:root:Error in attempt 3: 'int' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw response from Claude: Here is the key information extracted from the research paper in JSON format:\n",
            "\n",
            "{\n",
            "    \"PaperTitle\": \"Hummelsheim procedure combined with medial rectus recession in complete sixth nerve palsy and esotropic Duane Retraction Syndrome\",\n",
            "    \"patientsNo\": \"39\",\n",
            "    \"patientsINFO\": [\n",
            "        {\n",
            "            \"Diagnose\": \"Complete sixth nerve palsy (CSNP)\",\n",
            "            \"patientsNoAHP\": \"22 patients\",\n",
            "            \"AHPType\": \"head turn\",\n",
            "            \"AHPDirection\": \"\", \n",
            "            \"AHPDegree\": \"34.3 ± 8.4 degrees\",\n",
            "            \"eye\": \"22 unilateral cases\", \n",
            "            \"eyeMisalignment\": \"esotropia\",\n",
            "            \"degree\": \"45.8 ± 22 PD\"\n",
            "        },\n",
            "        {\n",
            "            \"Diagnose\": \"Esotropic Duane Retraction Syndrome (eDRS)\",\n",
            "            \"patientsNoAHP\": \"17 patients\", \n",
            "            \"AHPType\": \"head turn\",\n",
            "            \"AHPDirection\": \"\",\n",
            "            \"AHPDegree\": \"26.5 ± 9 degrees\",  \n",
            "            \"eye\": \"17 unilateral cases\",\n",
            "            \"eyeMisalignment\": \"esotropia, globe retraction\",\n",
            "            \"degree\": \"22.5 ± 6.4 PD, 1.86 ± 0.69 grade\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "Raw response from Claude: {\n",
            "    \"PaperTitle\": \"Bilateral familial vertical Duane Syndrome with synergistic convergence, aberrant trigeminal innervation, and facial hypoplasia\",\n",
            "    \"patientsNo\": \"1\"\n",
            "}\n",
            "\n",
            "{\n",
            "    \"Diagnose\": \"Bilateral Familial Vertical Duane Retraction Syndrome\",\n",
            "    \"patientsNoAHP\": \"1 patient\",\n",
            "    \"patientsInfo\": {\n",
            "        \"Diagnose\": \"Bilateral Familial Vertical Duane Retraction Syndrome\",\n",
            "        \"patientsNoAHP\": \"1 patient\",\n",
            "        \"AHPType\": \"chin elevation, head turn, head tilt\",\n",
            "        \"AHPDirection\": \"right\",\n",
            "        \"AHPDegree\": \"\", \n",
            "        \"eye\": \"both\",\n",
            "        \"eyeMisalignment\": \"esotropia\",\n",
            "        \"degree\": \">90 PD with V pattern\"\n",
            "    }\n",
            "}\n",
            "\n",
            "The key points I extracted are:\n",
            "\n",
            "1. The paper is a case report on a single 5-year-old patient. \n",
            "\n",
            "2. The main diagnosis is Bilateral Familial Vertical Duane Retraction Syndrome.\n",
            "\n",
            "3. The patient had an abnormal head posture of chin elevation, right head turn, and mild right head tilt.\n",
            "\n",
            "4. Both eyes were affected, with a large angle alternating esotropia of >90 prism diopters with a V pattern.\n",
            "\n",
            "5. Other clinical details like degree of head posture, eye misalignment measurements besides esotropia were not clearly mentioned.\n",
            "\n",
            "Let me know if you need any clarification or have additional requirements!\n",
            "Raw response from Claude: {\n",
            "    \"PaperTitle\": \"Bilateral familial vertical Duane Syndrome with synergistic convergence, aberrant trigeminal innervation, and facial hypoplasia\",\n",
            "    \"patientsNo\": \"1\"\n",
            "}\n",
            "\n",
            "{\n",
            "    \"Diagnose\": \"Vertical Duane Retraction Syndrome\",\n",
            "    \"patientsNoAHP\": \"1 patient\"\n",
            "}\n",
            "\n",
            "{  \n",
            "    \"patientsInfo\": {\n",
            "        \"Diagnose\": \"Vertical Duane Retraction Syndrome\", \n",
            "        \"patientsNoAHP\": \"1 patient\",\n",
            "        \"AHPType\": \"chin elevation, head turn, head tilt\",\n",
            "        \"AHPDirection\": \"right\",\n",
            "        \"AHPDegree\": \"\",\n",
            "        \"eye\": \"both\",\n",
            "        \"eyeMisalignment\": \"esotropia\",\n",
            "        \"degree\": \">90 PD\"\n",
            "    }\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Failed to process 102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw response from Claude: {\n",
            "    \"PaperTitle\": \"Bilateral familial vertical Duane Syndrome with synergistic convergence, aberrant trigeminal innervation, and facial hypoplasia\",\n",
            "    \"patientsNo\": \"1\"\n",
            "}\n",
            "\n",
            "{\n",
            "    \"Diagnose\": \"Duane Retraction Syndrome (DRS)\",\n",
            "    \"patientsNoAHP\": \"1 patient\"\n",
            "}\n",
            "\n",
            "{  \n",
            "    \"patientsInfo\": {\n",
            "        \"Diagnose\": \"Duane Retraction Syndrome (DRS)\", \n",
            "        \"patientsNoAHP\": \"1 patient\",\n",
            "        \"AHPType\": \"chin elevation, head turn, head tilt\",\n",
            "        \"AHPDirection\": \"right\",\n",
            "        \"eye\": \"both\",\n",
            "        \"eyeMisalignment\": \"esotropia\",\n",
            "        \"degree\": \">90 PD\"\n",
            "    }\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error in attempt 2: 'int' object is not subscriptable\n",
            "ERROR:root:Error in attempt 3: 'int' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw response from Claude: Here is the extracted information in JSON format:\n",
            "\n",
            "{\n",
            "    \"PaperTitle\": \"How Successful is Combined Superior and Inferior Oblique Muscle Surgery in Young Children with Superior Oblique Underaction Presenting in Infancy with a Severe Head Tilt?\",\n",
            "    \"patientsNo\": \"5\",\n",
            "    \"patientsINFO\": [\n",
            "        {\n",
            "            \"Diagnose\": \"Right Superior Oblique Palsy (SOP)\",\n",
            "            \"patientsNoAHP\": \"1 patient\",\n",
            "            \"AHPType\": \"head tilt\",\n",
            "            \"AHPDirection\": \"left\",\n",
            "            \"AHPDegree\": \"35 degrees\",\n",
            "            \"eye\": \"right\",\n",
            "            \"eyeMisalignment\": \"hypertropia\",\n",
            "            \"degree\": \"30 PD\"\n",
            "        },\n",
            "        {\n",
            "            \"Diagnose\": \"Right SOP\",\n",
            "            \"patientsNoAHP\": \"1 patient\",\n",
            "            \"AHPType\": \"head tilt\",\n",
            "            \"AHPDirection\": \"left\", \n",
            "            \"AHPDegree\": \"32 degrees\",\n",
            "            \"eye\": \"right\",\n",
            "            \"eyeMisalignment\": \"hypertropia\",\n",
            "            \"degree\": \"30 PD\"\n",
            "        },\n",
            "        {\n",
            "            \"Diagnose\": \"Left SOP\",\n",
            "            \"patientsNoAHP\": \"1 patient\",\n",
            "            \"AHPType\": \"head tilt\",\n",
            "            \"AHPDirection\": \"right\",\n",
            "            \"AHPDegree\": \"35 degrees\", \n",
            "            \"eye\": \"left\",\n",
            "            \"eyeMisalignment\": \"hypertropia\",\n",
            "            \"degree\": \"25 PD\"\n",
            "        },\n",
            "        {\n",
            "            \"Diagnose\": \"Right SOP\",\n",
            "            \"patientsNoAHP\": \"1 patient\",\n",
            "            \"AHPType\": \"head tilt\",\n",
            "            \"AHPDirection\": \"left\",\n",
            "            \"AHPDegree\": \"20 degrees\",\n",
            "            \"eye\": \"right\",\n",
            "            \"eyeMisalignment\": \"hypertropia\",\n",
            "            \"degree\": \"25 PD\"\n",
            "        },\n",
            "        {\n",
            "            \"Diagnose\": \"Right SOP\",\n",
            "            \"patientsNoAHP\": \"1 patient\", \n",
            "            \"AHPType\": \"head tilt\",\n",
            "            \"AHPDirection\": \"left\",\n",
            "            \"AHPDegree\": \"30 degrees\",\n",
            "            \"eye\": \"right\",\n",
            "            \"eyeMisalignment\": \"hypertropia\",\n",
            "            \"degree\": \"25 PD\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "\n",
            "Comparison Summary:\n",
            "Total Files: 5\n",
            "Matched Files: 5\n",
            "Overall Accuracy: 35.45%\n",
            "\n",
            "Field-level Accuracy:\n",
            "PaperTitle: 77.27% (17/22)\n",
            "patientsNo: 0.00% (0/22)\n",
            "Diagnose: 40.91% (9/22)\n",
            "patientsNoAHP: 0.00% (0/22)\n",
            "AHPType: 63.64% (14/22)\n",
            "AHPDirection: 63.64% (14/22)\n",
            "AHPDegree: 13.64% (3/22)\n",
            "Eye: 31.82% (7/22)\n",
            "EyeMisalignment: 31.82% (7/22)\n",
            "DegreePD: 31.82% (7/22)\n",
            "\n",
            "Detailed comparison saved to: /content/drive/MyDrive/Body-Language-Project/Data Extraction/Extracted JSON/Claude Sonnet 3.5/Hierarchical Prompt/detailed_comparison.csv\n",
            "Summary report saved to: /content/drive/MyDrive/Body-Language-Project/Data Extraction/Extracted JSON/Claude Sonnet 3.5/Hierarchical Prompt/comparison_summary.json\n",
            "\n",
            "Processing Complete!\n",
            "Total papers processed: 5\n",
            "Successful extractions: 3\n",
            "Failed extractions: 2\n",
            "Overall extraction accuracy: 28.33%\n",
            "\n",
            "Output files:\n",
            "1. JSON results: /content/drive/MyDrive/Body-Language-Project/Data Extraction/Extracted JSON/Claude Sonnet 3.5/Hierarchical Prompt\n",
            "2. Combined CSV: /content/drive/MyDrive/Body-Language-Project/Data Extraction/Extracted JSON/Claude Sonnet 3.5/Hierarchical Prompt/standardized_extracted_data.csv\n",
            "3. Comparison results: /content/drive/MyDrive/Body-Language-Project/Data Extraction/Extracted JSON/Claude Sonnet 3.5/Hierarchical Prompt/detailed_comparison.csv\n",
            "4. Summary report: /content/drive/MyDrive/Body-Language-Project/Data Extraction/Extracted JSON/Claude Sonnet 3.5/Hierarchical Prompt/comparison_summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "61bzVWq2MrEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dskO3XLS53XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "obmcO5ejMrBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QUEgYrOnMq_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ya7jxdVT4QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dhfpUL3cZFUP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}